---
title: "TMRC3 `r Sys.getenv('VERSION')`: README/Introduction"
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
output:
 html_document:
  code_download: true
  code_folding: show
  fig_caption: true
  fig_height: 7
  fig_width: 7
  highlight: default
  keep_md: false
  mode: selfcontained
  number_sections: true
  self_contained: true
  theme: readable
  toc: true
  toc_float:
   collapsed: false
   smooth_scroll: false
---

# Introduction

The R markdown documents in this directory are intended to provide a
complete accounting of the analyses performed in the preparation of

"Innate biosignature of treatment failure in patients with cutaneous
leishmaniasis."

I assume that if anyone ever reads this, s/he is looking for the
source of the data, figures, and tables from that paper and to see if
the methods employed to generate them are good, bad, indifferent, or
garbage.  These documents are being generated by a singularity
container which provides the input count tables (Once we have
accessions, I will make a companion which is able to create them),
sample sheets, input documents, and all the software used to process
them.  The overal configuration of the container is contained in the
toplevel .yml file and provides the base OS (Debian stable) and the
scripts used to install the software.  'local/bin/setup_debian.sh'
handles the base container; 'local/bin/setup_hpgltools.sh' sets up R,
bioconductor, and my package 'hpgltools', and 'local/bin/runscript' is
the script run if one invokes the container without any arguments;
when run it uses the various installed R packages to 'render' the Rmd
files in /data to freshly created html.  This README is the first
document rendered.

# Structure

The Rmd files may be rendered in any order with one very important
exception: the 01datastructures.Rmd file must be run first.  It reads
all of the sample sheets and count tables and writes out a series of
data files used by all the other documents.

## 01datastructures

This is responsible for setting the stage for everything that follows.
It collects annotations from the 2020 ensembl human database, the
experimental metadata from the xlsx files in sample_sheets/, and the
counts found in preprocessing/ and combines them into an initial,
large expressionSet.  It tallies up the samples according to many/most
of the likely factors of interest, filters the data, and extracts the
various subsets into separate datastructures.

### Figure 1: Participant recruitment and study design

The numbers of people involved in the study may be found throughout
this document, primarily in the sections with titles starting with:
'Summarize'.

### Figure S2: Evaluation of sequencing depth per sample

This figure is about a quarter of the way down in an eponymously named
block.  A svg format version of that image is created in the figures/
directory in this location.  The datastructure used is named
'tc_valid', because it contains both the Tumaco and Cali data (tc)
after removing the most troublesome samples (valid).

### Figure 2A: Global gene expression profiles of blood cells

The numbers of samples in each group are restated in the summaries.
The only real caveat is that I wrote them as 'visit1' or 'v1' for
Pre-Tx, 'visit2' for Mid-Tx, and 'visit3' for End-Tx.

One way to recapitulate these numbers is to check out the sankey plots
in section 8 'Visualize the sample breakdown'.  In addition, there is
an explicit counting of the samples by celltype/time/CF in section 11
'Summarize: tabulate sample numbers.'  Looking at the Rmd file in this
section; the numbers are all generated by querying the number of
samples in each subset datastructure followed by counting up the
number of cures/fails.

## 02visualization

This document is dominated by a long series of PCA explorations.  It
is basically a playground for me to poke at the various data subsets
in order to try to get a feeling for what is the most appropriate way
to think about the data.  It was where we eventually decided that we
cannot use the Cali data, for example.

### Figure 2B/C

This should be found in the eponymously named section.  The 'tc_valid'
datastructure was normalized via log2(quant(cpm(filter())))ing and
a pairwise correlation heatmap and PCA were plotted.  Weirdly, I do
not have an explicit printing of the SVG version of these panels, I
think because we opened them post-facto in inkscape to move around the
legends.

### Figure S4A/B/C

These begin at section 7.4.1.4.  I just realized I changed the color
scheme for these because they were not showing up on my screen with
the black/grey/white colors!  I need to go back and change them
back to the greyscale so they match.

# Introduction to the singularity container

Of the various options, I found singularity to be the most attractive.
I therefore wrote a default Makefile with a few targets to create and
manipulate images because I cannot be bothered to remember all of the
various commands.

# The images

I am hoping to create 2 image templates for the TMRC analyses:
preprocessing and analyses. I will therefore have a few locally
maintained shell scripts which contain the base image setup tasks, the
tasks required to setup the tools used, download the data, and perform
the work.

# Setting up the base image

I intend to use Debian stable.  I am copying the required setup files
into the current working directory and invoking make to create
the container.  I have a few targets which are intended to make this
easier to remember.

## Creating the base image

The following command runs singularity with options suitable to create
the image.  This to me is a little unnerving, because a bunch of stuff
gets run as root.

```{bash, eval=FALSE}
make
```

## Testing stuff out and making changes

The overlay target drops the user into the container with R/W
permissions.  It creates a directory 'cure_Fail_analyses_overlay' which
may be modified at will.

```{bash, eval=FALSE}
make cure_fail_analyses.overlay
```

## Using the container for arbitrary Rmd/md files

The container's runscript has some logic built in which can process
arbitrary markdown documents into html via knitr and pandoc.

```{bash, eval=FALSE}
cd someplace_with_Rmd
/locationofcontainers/cure_fail_host_analyses.sif -i markdown01.Rmd:markdown02.Rmd
```
