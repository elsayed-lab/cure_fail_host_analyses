---
title: "TMRC3 ML Classification of outcome: `r Sys.getenv('VERSION')`"
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
bibliography: atb.bib
output:
  html_document:
    code_download: true
    code_folding: show
    fig_caption: true
    fig_height: 7
    fig_width: 7
    highlight: zenburn
    keep_md: false
    mode: selfcontained
    number_sections: true
    self_contained: true
    theme: readable
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

<style type="text/css">
body .main-container {
  max-width: 1600px;
}
body, td {
  font-size: 16px;
}
code.r{
  font-size: 16px;
}
pre {
  font-size: 16px
}
</style>

```{r, include=FALSE}
library(hpgltools)
library(caret)
library(dplyr)
library(pROC)
library(DALEX)
library(glmnet)
library(glue)
library(kernlab)
library(ranger)
library(xgboost)
library(ggstatsplot)

knitr::opts_knit$set(progress = TRUE, verbose = TRUE, width = 90, echo = TRUE)
knitr::opts_chunk$set(
  error = TRUE, fig.width = 8, fig.height = 8, fig.retina = 2,
  out.width = "100%", dev = "png",
  dev.args = list(png = list(type = "cairo-png")))
old_options <- options(digits = 4, stringsAsFactors = FALSE, knitr.duplicate.label = "allow")
ggplot2::theme_set(ggplot2::theme_bw(base_size = 12))
ver <- Sys.getenv("VERSION")
rundate <- format(Sys.Date(), format = "%Y%m%d")

rmd_file <- "06classifier_highvar.Rmd"
savefile <- gsub(pattern = "\\.Rmd", replace = "\\.rda\\.xz", x = rmd_file)
loaded <- load(file = glue("rda/tmrc3_data_structures-v{ver}.rda"))
```

# Changelog

* 202408: Collapsing the various ML documents back into one in the
  hopes that it eases the degree of confusion.  All of the exploratory
  analyses are at the top, at the bottom are the blocks which are used
  to generate the final tables, but they are only runnable manually
  due to time constraints.
* 202408: Re-enabled glm, it seems to be running faster again?
* 20240620: Disabled glm training/testing in the container.  For
  reasons I have not yet figured out it takes ~ 10x longer (~10 days)
  in the container (note, this is a new thing, it used to take ~ 8
  hours in the container without error, so something changed in
  202405-202406 caret/glmnet/singularity/etc) to complete; which is a
  problem because my notebooks are auto-regenerated on a 2x/week basis
  (if changes occur) and so it doesn't finish before another round
  starts...
* 202309: Adding a version of this with only the Tumaco data.  I am a
  pretty big non-fan of doing this; so I am going to do so in the most
  cursory fashion possible by just copying this document to another copy with
  a suffix of 'tumaco' and changing the initial datastructure to the Tumaco-only
  dataset.


# Introduction

I had some success in classifying the TMRC2 samples by strain via ML
and want to try something more difficult.  Thus I will use the
normalized gene expression data and try classifying it by cure/fail.

All analyses in this document heavily use caret
(@kuhnBuildingPredictiveModels2008) and, as per the sister TMRC2
document, follow pretty closely the path from:

https://topepo.github.io/caret/data-splitting.html#simple-splitting-based-on-the-outcome

and

https://github.com/compgenomr/book/blob/master/05-supervisedLearning.Rmd

# Starter data

In the strain classifier I used normalized variants.  I am thinking to
use normalized expression here and therefore explicitly limit myself
to ~ 20k variables (significantly down from the 1.6M).

In addition, caret expects the data as (rows == samples) and
(columns == variables) where each element is one observation.
Thus we will need to transpose the expression matrix.

```{r}
input_data <- subset_expt(tc_clinical, subset="batch!='biopsy'") %>%
  normalize_expt(transform = "log2", convert = "cpm")

ref_col <- "finaloutcome"
outcome_factor <- as.factor(as.character(pData(input_data)[[ref_col]]))
comparison_n <- 200

## Setting this up in case Najib and Maria Adelaida request a Tumaco set of exploratory analyses.
t_input_data <- subset_expt(tc_clinical, subset="batch!='biopsy'") %>%
  normalize_expt(transform = "log2", convert = "cpm")
t_outcome_factor <- as.factor(as.character(pData(t_input_data)[[ref_col]]))
```

# Compare TopN important genes to DE enes

Given that we have separated the various analyses, it will take me a
minute to figure out where I saved the relevant differential
expression analysis.  I do not actually save the various DE results to
rda files by default, instead opting to send them to xlsx files to
share.  Recall if you will, that the data that I think might be used
for the paper also does not go into the default excel directory but
instead mirrors the box organization scheme.

Thus, I think the most relevant file is:
"analyses/3_cali_and_tumaco/DE_Cure_vs_Fail/All_Samples/tc_cf_clinical_table_sva-v202207.xlsx"

and

"analyses/4_tumaco/DE_Cure_vs_Fail/All_Samples/t_cf_clinical_table_sva-v202207.xlsx"

Note, this xlsx file contains three primary worksheets, v1cf, v2cf,
v3cf which comprise all samples across cell types for each visit and
compares cure and fail.  When I choose sheet #2 in the following block
I am explicitly asking for only the v1 cure/fail result.

## Opening the relevant DE table

```{r}
input_xlsx <- glue("analyses/3_cali_and_tumaco/DE_Cure_Fail/Clinical_Samples/tc_clinical_cf_sig_sva-v{ver}.xlsx")
all_de_cf <- openxlsx::readWorkbook(input_xlsx, sheet = 3, startRow = 2)
rownames(all_de_cf) <- all_de_cf[["row.names"]]
all_de_cf[["row.names"]] <- NULL
deseq_de_cf_up <- all_de_cf[, c("deseq_logfc", "deseq_adjp", "deseq_basemean", "deseq_lfcse")]
all_de_cf <- openxlsx::readWorkbook(input_xlsx, sheet = 4, startRow = 2)
rownames(all_de_cf) <- all_de_cf[["row.names"]]
all_de_cf[["row.names"]] <- NULL
deseq_de_cf_down <- all_de_cf[, c("deseq_logfc", "deseq_adjp", "deseq_basemean", "deseq_lfcse")]
deseq_de_cf <- rbind.data.frame(deseq_de_cf_up, deseq_de_cf_down)

tumaco_xlsx <- glue("analyses/4_tumaco/DE_Cure_Fail/t_all_visitcf_sig_sva-v{ver}.xlsx")
t_de_cf <- openxlsx::readWorkbook(tumaco_xlsx, sheet = 3, startRow = 2)
rownames(t_de_cf) <- t_de_cf[["row.names"]]
t_de_cf[["row.names"]] <- NULL
t_de_cf_up <- t_de_cf[, c("deseq_logfc", "deseq_adjp", "deseq_basemean", "deseq_lfcse")]
t_de_cf <- openxlsx::readWorkbook(tumaco_xlsx, sheet = 4, startRow = 2)
rownames(t_de_cf) <- t_de_cf[["row.names"]]
t_de_cf[["row.names"]] <- NULL
t_de_cf_down <- t_de_cf[, c("deseq_logfc", "deseq_adjp", "deseq_basemean", "deseq_lfcse")]
t_de_cf <- rbind.data.frame(t_de_cf_up, t_de_cf_down)
top_bottom_ids <- rownames(t_de_cf)
```

## A function to help compare methods to DESeq2

The following block contains a few lists which will hold comparisons
between DESeq2 and the various methods which follow, some parameters,
and a function to compare the methods and DESeq.

Presumably DESeq and the models should be responding to variance in
the data, for which I think the logFC values, p-values, mean values,
or standard errors are the most likely proxies to which I have easy
access.  So, let us pull the top/bottom n genes vis a vis each of
those categories and see what happens?

Here is a little function to compare a ML result to the above DE
table.  It should provide the importance plot, a venn of the two
methods, and the gene sets (with annotations) for the unique/shared
genes between this ML method and the DE table.  Note, this is just
using the highest/lowest logFC values and ignoring the p-value for now.

```{r}
comparison_lfc <- list()
comparison_adjp <- list()
comparison_wgcna <- list()
annotations <- fData(t_monocytes)  ## Arbitrarily taken

compare_ml_de <- function(importance, sig_table, annotations, n = comparison_n, plot_n = 15,
                          according_to = "deseq_logfc", column = "Overall") {
  ml_df <- importance[["importance"]] %>%
    arrange(desc(!!sym(column)))
  importance_plot <- plot(importance, top = plot_n)
  top_ml <- head(rownames(ml_df), n = n)
  de_df <- sig_table %>%
    arrange(desc(deseq_logfc))
  top_de <- head(rownames(de_df), n = n)
  bottom_de <- tail(rownames(de_df), n = n)
  top_bottom_ids <- c(top_de, bottom_de)
  comparison <- list("de" = top_bottom_ids, "ml" = top_ml)
  comparison_venn <- Vennerable::Venn(comparison)
  shared_ids <- comparison_venn@IntersectionSets["11"][[1]]
  shared_annot <- annotations[shared_ids, ]
  de_ids <- comparison_venn@IntersectionSets["10"][[1]]
  ml_ids <- comparison_venn@IntersectionSets["01"][[1]]
  only_de <- annotations[de_ids, ]
  only_ml <- annotations[ml_ids, ]
  comparison_plot <- Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
  retlist <- list(
    "importance_plot" = importance_plot,
    "comparison" = comparison,
    "comparison_venn" = comparison_venn,
    "comparison_plot" = comparison_plot,
    "shared" = shared_annot,
    "only_de" = only_de,
    "only_ml" = only_ml)
  return(retlist)
}
```

# Filtering

The ML text I am reading provide some neat examples for how one might
filter the data to make it more suitable for model creation.

## Near zero variance, or genefilter's cv

The first filter I was introduced to is quite familiar from our
sequencing data, the removal of features with near-zero-variance.
Indeed, I am pretty certain that normalize_expt() could do this
equivalently and significantly faster than caret::preProcess().

```{r}
nrow(exprs(input_data))
system.time({
  equivalent <- normalize_expt(input_data, filter = "cv", cv_min = 0.1)
})
dim(exprs(equivalent))

## Given this large amount of data, this step is slow, taking > 10 minutes.
## Yeah seriously, the following three lines get 16,723 genes in 10 minutes while
## the normalize_expt() call above gets 16,749 genes in 2.4 seconds.
#system.time({
#  nzv <- preProcess(texprs, method="nzv", uniqueCut=15)
#  nzv_texprs <- predict(nzv, texprs)
#  dim(nzv_texprs)
#}
nzv_texprs <- t(exprs(equivalent))
```

## Filtering to the highest standard deviation variables

I think I am a bit confused by this filter, one would think that the
nzv filter above, if applied correctly, should give you exactly this.

For the moment, I am excluding the following block in order to see how
much time/memory keeping these variables costs.  If I recall properly,
the model of the top-2k variant positions cost ~ 1-4G of memory.  I
hope that this scales linearly, but I am thinking it might not.

```{r}
standard_devs <- apply(nzv_texprs, 2, sd)
top_predictors <- order(standard_devs, decreasing = TRUE)[1:3000]
nzv_texprs <- nzv_texprs[, top_predictors]
```

## Center the data

I think centering may not be needed for this data, but here is how:

```{r}
nzv_center <- preProcess(nzv_texprs, method = "center")
nzv_texprs <- predict(nzv_center, nzv_texprs)
```

## Drop correlated

This is a filter which does not correspond to any of those we use in
sequencing data because genes which are highly correlated are
likely to be of immediate interest.

In the same fashion, I want to leave this off because later
applications of this model will include low coverage samples which may
not have every variant represented.

```{r}
## This step takes a while...
system.time({
  nzv_correlated <- preProcess(nzv_texprs, method = "corr", cutoff = 0.95)
  nzv_uncorr <- predict(nzv_correlated, nzv_texprs)
})
dim(nzv_uncorr)
```

# Merge the appropriate metadata

There are a few metadata factors which might prove of interest for
classification.  The most obvious are of course outcome, clinic,
donor, visit, celltype.  I am, for the moment, only likely to focus on
outcome.  AFAICT I can only include one of these at a time in the
data, which is a shame.

```{r}
interesting_meta <- pData(input_data)[, c("finaloutcome", "donor", "persistence",
                                          "visitnumber", "selectionmethod",
                                          "typeofcells", "time")]

ml_df <- as.data.frame(cbind(outcome_factor, as.data.frame(nzv_uncorr)))
ml_df[["outcome_factor"]] <- as.factor(ml_df[["outcome_factor"]])
dim(ml_df)
```

# Split the data into training/testing

caret provides nice functionality for splitting up the data.  I
suspect there are many more fun knobs I can play with for instances
where I need to exclude some levels of a factor and such.  In this
case I just want to split by outcome.

## Via data splitting

```{r}
ml_df <- as.data.frame(cbind(outcome_factor, as.data.frame(nzv_uncorr)))

datasets <- create_partitions(nzv_uncorr, interesting_meta,
                              outcome_factor = outcome_factor)
```

## Via sampling

There are a few likely sampling methods: cross-validation,
bootstrapping, and jackknifing.  I will try those out later.

# Try out training and prediction methods

My goals from here on will be to get the beginnings of a sense of the
various methods I can use to create the models from the training data
and predict the outcome on the test data.  I am hoping also to pick up
some idea of what the various arguments mean while I am at it.

## Try out KNN

k-nearest neighbors is somewhat similar to a kmeans estimate.  Thus
the primary argument is 'k'

### Model creation and performance

```{r}
split <- 1
train_all <- datasets[["trainers"]][[split]]
train_df <- datasets[["trainers_stripped"]][[split]]
train_idx <- datasets[["train_idx"]][[split]]
train_outcomes <- datasets[["trainer_outcomes"]][[split]]
test_df <- datasets[["testers"]][[split]]
test_idx <- datasets[["test_idx"]][[split]]
test_outcomes <- datasets[["tester_outcomes"]][[split]]

knn_fit <- knn3(x = train_df,
                y = train_outcomes,
                k = 3)
knn_predict_trained <- predict(knn_fit, train_df, type = "prob")

knn_train_evaluated <- self_evaluate_model(knn_predict_trained, datasets,
                                           which = split, type = "train")
knn_train_evaluated
```

As the confusion matrix shows, this failed for a few samples.  Perhaps
let us change k and see if it improves.

Here is a table of fase positives/negatives for a few values of 'k',
in this context a false positive is calling a known cure as a failure
and false negative is calling a known failure as a cure.

|---|---|---|
|k  |fp |fn |
|2  |0  |8  |
|3  |5  |5  |
|4  |8  |9  |
|5  |11 |7  |
|6  |15 |8  |

Note: this depends on the luck of rand(), so the above numbers shift
moderately from one run to the next.  Thus I think I will just use 2
or 3.

```{r}
knn_fit2 <- knn3(x = train_df,
                y = train_outcomes,
                k = 5)
knn_predict_trained2 <- predict(knn_fit2, train_df, type = "prob")

knn_train_evaluated2 <- self_evaluate_model(knn_predict_trained2, datasets,
                                            which = split, type = "train")
knn_train_evaluated2
```

### Predict the rest of the data with this model.

```{r}
knn_predict_test <- predict(knn_fit, test_df)

knn_test_evaluated <- self_evaluate_model(knn_predict_test, datasets,
                                     which = split, type = "test")
knn_test_evaluated

knn_predict_test2 <- predict(knn_fit2, test_df)
knn_test_evaluated2 <- self_evaluate_model(knn_predict_test2, datasets,
                                           which = split, type = "test")
knn_test_evaluated2
```

## Perform cross-validation to estimate k

The cross validation method of repeated sampling the data is all done
within the train() function.  With that in mind, here it is operating
with the knn method.

### CV with knn

When train() is called with the trControl and tuneGrid, we can control
how the knn training is repeated, in this case it will iterate over k
from 1 to 10.

This currently fails due to a stack overflow...

```{r, eval=FALSE}
cv_control <- trainControl(method = "cv", number = 10)

knn_train_fit <- train(outcome_factor ~ ., data = train_df,
                       method = "knn",
                       trControl = cv_control,
                       tuneGrid = data.frame(k = 1:10))
knn_train_fit[["bestTune"]]

plot(x = 1:10, 1 - knn_train_fit$results[, 2], pch = 19,
     ylab = "prediction error", xlab = "k")
lines(loess.smooth(x = 1:10, 1 - knn_train_fit$results[, 2],degree = 2),
      col = "#CC0000")
```

### Bootstrap with knn

```{r}
boot_control <- trainControl(method = "boot", number = 20,
                             returnResamp = "all")

knn_train_fit <- train(outcome ~ ., data = train_all,
                       method = "knn",
                       trControl = boot_control,
                       tuneGrid = data.frame(k = 1:10))
knn_train_fit[["bestTune"]]

plot(x = 1:10, 1 - knn_train_fit$results[, 2], pch = 19,
     ylab = "prediction error", xlab = "k")
lines(loess.smooth(x = 1:10, 1 - knn_train_fit$results[, 2],degree = 2),
      col = "#CC0000")
```

### Explain the important variables

In this instance we will search for genes which were important for the
model's creation.

The DALEX package provides a function: feature_importance() which
seeks to use a series of other methods to extract (in this case,
genes) features which do a good job of explaining the result produced
by the model.  In the case of this dataset, which has thousands of
features, this does not appear to end well.

```{r, eval=FALSE}
explainer_knn <- DALEX::explain(knn_fit, label = "knn",
                                data = train_df,
                                y = as.numeric(train_outcomes))

## AFAICT the following will take forever unless we drastically reduce the complexity of the model.
## yeah, I let it run for a week.
## features <- feature_importance(explainer_knn, n_sample = 50, type = "difference")
```

Conversely, we can use the varImp() function...

```{r}
knn_variable_importance <- varImp(knn_train_fit)
plot(knn_variable_importance, top = 15)
knn_variables <- knn_variable_importance[["importance"]] %>%
  arrange(desc(cure))
```

Given that, we can ask for the similarity to the DESeq results...

```{r}
knn_comparison <- compare_ml_de(knn_variable_importance, deseq_de_cf, annotations,
                                n = comparison_n, plot_n = 15, column = "cure")
knn_comparison[["comparison_plot"]]
comparison_lfc[["knn"]] <- knn_comparison[["shared"]]
```

## Random Forest

I think R/caret provides a few implementation of the forest family of
methods, this is using ranger(@wrightRangerFastImplementation2017a).

The parameter 'mtry' is often important, if I read the text correctly
it controls how many variables to sample in each split of the tree.
Thus higher numbers should presumably make it more specific at the
risk of overfitting.

Setting min.node.size sets the minimume node size of terminal nodes in
each tree.  Each increment up speeds the algorithm.

I am going to use my boot control trainer from above and see how it goes.

```{r}
rf_train_fit <- train(outcome ~ ., data = train_all,
                method = "ranger", trControl = boot_control,
                importance = "permutation",
                tuneGrid = data.frame(
                    mtry = 200,
                    min.node.size = 1,
                    splitrule = "gini"),
                verbose = TRUE)
rf_train_fit[["finalModel"]][["prediction.error"]]

rf_variable_importance <- varImp(rf_train_fit)
plot(rf_variable_importance, top = 15)
rf_variables <- rf_variable_importance[["importance"]] %>%
  arrange(desc(Overall))

rf_predict_trained <- predict(rf_train_fit, train_df)
rf_predict_evaluated <- self_evaluate_model(rf_predict_trained, datasets,
                                            which = split, type = "train")
rf_predict_evaluated
```

### What would be shared between DESeq2 and the ML classifier?

```{r}
rf_comparison <- compare_ml_de(rf_variable_importance, deseq_de_cf, annotations,
                               n = comparison_n, plot_n = 15)
rf_comparison[["comparison_venn"]]
comparison_lfc[["rf"]] <- rf_comparison[["shared"]]
```

### Compare to the WGCNA results

A couple months ago I spent a little time attempting to recapitulate
Alejandro's WGCNA results.  I think I did so by mostly copy/pasting
his work and adding some commentary and tweaking parts of it so that
it was easier for me to read/understand.  In the process, I generated
a series of modules which looked similar/identical to his.
Unfortunately, I did not add some sections to record the genes/modules
to some output files.  I am therefore going back to that now and doing
so in the hopes that I can compare those modules to the results
produced by the clasifiers.

It seems that the interest in this comparison has waned, so I am just
going to disable it.

```{r, eval=FALSE}
wgcna_result <- openxlsx::readWorkbook(glue("excel/wgcna_interesting_genes-v{ver}.xlsx"))
rownames(wgcna_result) <- wgcna_result[["row.names"]]
wgcna_result[["row.names"]] <- NULL

top_ml <- rownames(head(rf_variables, n = comparison_n))
comparison <- list("wgcna" = rownames(wgcna_result), "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_wgcna[["rf"]] <- annotations[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

#### Digression do the genes provided by varImp mean anything?

Let us take a moment and see if the top-n genes returned by varImp()
have some meaning which jumps out.  One might assume, given our extant
Differential Expression results, that the interleukin response will be
a likely candidate.

```{r}
importance_gp <- simple_gprofiler(rownames(head(rf_variables, n = comparison_n)))
importance_gp
```

### Now the random forest testers!

```{r}
rf_predict_test <- predict(rf_train_fit, test_df)

rf_predict_test_evaluated <- self_evaluate_model(rf_predict_test, datasets,
                                     which = split, type = "test")
rf_predict_test_evaluated
```

## GLM, or Logistic regression and regularization

Logistic regression is a statistical method for binary responses.
However, it is able to work with multiple classes as well.  The
general idea of this method is to find parameters which increase the
likelihood that the observed data is sampled from a statistical
distribution of interest.  The transformations and linear
regression-esque tasks performed are confusing, but once those are
performed, the task becomes setting the model's (fitting) parameters
to values which increase the probability that the statistical model
looks like the actual dataset given the training data, and that when
samples, will return values which are similar.  The most likely
statistical distributions one will want to fit are the Gaussian, in
which case we want to transform/normalize the mean/variance of our
variables so they look whatever normal distribution we are using.
Conversely, logistic regression uses a binnomial distribution (like
our raw sequencing data!) but which is from 0-1.

### Using a single gene

Let us take the most important gene observed in one of our previous
training sets: ENSG00000248405 PRR5-ARHGAP8

```{r, eval=FALSE}
gene_id <- "ENSG00000248405"
single_fit <- train(
    outcome ~ ENSG00000248405, data = train_all,
    method = "glm", family = "binomial", trControl = trainControl("none"))

tt <- data.frame("ENSG00000248405" = seq(min(train_df[[gene_id]]),
                                         max(train_df[[gene_id]]), len = 100))
## predict probabilities for the simulated data
tt$subtype = predict(single_fit, newdata = tt, type="prob")[, 1]
## plot the sigmoid curve and the training data
plot(ifelse(outcome == "cure", 1, 0) ~ ENSG00000248405,
     data = train_all, col = "red4",
     ylab = "CF as 0 or 1", xlab = "favorite gene expression")
lines(subtype ~ ENSG00000248405, tt, col = "green4", lwd = 2)

plot_df <- train_all[, c("outcome", "ENSG00000248405")]
ggbetweenstats(plot_df, "outcome", "ENSG00000248405")
```

Having tried with 1 gene, let us extend this to all genes.  In my
first try of this, it took a long time.

```{r}
glm_train_fit <- train(outcome ~ ., data = train_all,
                 trControl = boot_control,
                 method = "glm", family = "binomial")
```

## Compare GLM and WGCNA/DE

The previous block does not take into account our parameter sweep, so
it isn't theoretically very useful for this series of methological
tests.  As a result, the following block is also not particularly
helpful -- though I suspect the results are identical to what follows.

```{r}
glm_variable_importance <- varImp(glm_train_fit)
## Oh, this only produces 100 entries -- so me getting the top 400 is silly.
glm_variables <- glm_variable_importance[["importance"]] %>%
  arrange(desc(Overall))
tt = plot(glm_variable_importance, top = 15)
simple_gprofiler(rownames(head(glm_variables, n = comparison_n)))

top_glm <- rownames(glm_variables)
comparison <- list("de" = top_bottom_ids, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
comparison_lfc[["glm"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

Dropping the adjp and wgcna comparisons, they show little overlap.

```{r, eval=FALSE}
comparison <- list("de" = lowest_adjp, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
comparison_adjp[["glm"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = highest_exprs, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

In this block, I repeat the above with the tuneGrid option set so that
it may do the training using a sweep of putatively helpful parameters;
as a result it should provide a more appropriate (but quite possibly
identical) trainer.

```{r}
##rf_method <- trainControl(method = "ranger", number = 10, verbose = TRUE)
## train_method <- trainControl(method = "cv", number = 10)
glm_fit <- train(outcome ~ ., data = train_all, method = "glmnet",
                 trControl = boot_control, importance = "permutation",
                 tuneGrid = data.frame(
                   alpha = 0.5,
                   lambda = seq(0.1, 0.7, 0.05)),
                 verbose = TRUE)
glm_fit

glm_predict_trained <- predict(glm_fit, train_df)

glm_train_eval <- self_evaluate_model(glm_predict_trained, datasets,
                                      which = split, type = "train")
glm_train_eval
```

### Now the GLM testers!

```{r}
glm_predict_test <- predict(glm_fit, test_df)

glm_fit_eval_test <- self_evaluate_model(glm_predict_test, datasets,
                                         which = split, type = "test")
glm_fit_eval_test
```

### Compare again vs the DE table

In the following block I extract the top and bottom DE genes vs. the
top set of genes in the glm_variables ordered by the 'Overall' column.

I should make this a function, there is no way this will kept correct
across multiple iterations of the ml/wgcna/etc comparisons.

I am going to stop trying to compare these results across
WGCNA/adjp/expression, those comparisons are all basically null.

```{r}
glm_variable_importance <- varImp(glm_fit)
glm_comparison <- compare_ml_de(glm_variable_importance, deseq_de_cf, annotations,
                                n = comparison_n, plot_n = 15)
comparison_lfc[["glm"]] <- glm_comparison[["shared"]]
```

## Gradient Booster

At this point I basically have a feeling for how one may use the
tuneGrid to apply a parameter sweep of the data along with the various
(in this case cv) sampling methods.  Thus, the following is basically
identical to the previous blocks except it is using xgbTree (which is
the R implementation of extreme boosting:
(@chenXGBoostScalableTree2016a)).

```{r}
##rf_method <- trainControl(method = "ranger", number = 10, verbose = TRUE)
train_method <- trainControl(method = "cv", number = 10)

gb_fit <- train(outcome ~ ., data = train_all,
                method = "xgbTree", trControl = train_method,
                tuneGrid = data.frame(
                    nrounds = 200,
                    eta = c(0.05, 0.1, 0.3),
                    max_depth = 4,
                    gamma = 0,
                    colsample_bytree = 1,
                    subsample = 0.5,
                    min_child_weight = 1),
                verbose = TRUE)

gb_predict_trained <- predict(gb_fit, train_df)
gb_predict_trained

gb_train_eval <- self_evaluate_model(gb_predict_trained, datasets,
                                     which = split, type = "train")
gb_train_eval
```

### Now the GB testers!

```{r}
gb_predict_test <- predict(gb_fit, test_df)

gb_predict_test_evaluated <- self_evaluate_model(gb_predict_test, datasets,
                                                 which = split, type = "test")
gb_predict_test_evaluated
```

```{r}
gb_variable_importance <- varImp(gb_fit)
gb_variables <- glm_variable_importance[["importance"]] %>%
  arrange(desc(Overall))
plot(gb_variable_importance, top = 15)
gb_comparison <- compare_ml_de(gb_variable_importance, deseq_de_cf, annotations, n = comparison_n, plot_n = 15)
comparison_lfc[["gb"]] <- gb_comparison[["shared"]]
```

# Shared importance

```{r}
upset_lfc <- list()
upset_adjp <- list()
upset_wgcna <- list()
for (d in 1:length(comparison_lfc)) {
  name <- names(comparison_lfc)[d]
  upset_lfc[[name]] <- rownames(comparison_lfc[[name]])
  upset_adjp[[name]] <- rownames(comparison_adjp[[name]])
  ##upset_wgcna[[name]] <- rownames(comparison_wgcna[[name]])
}

start_lfc <- UpSetR::fromList(upset_lfc)
lfc_vs_ml <- UpSetR::upset(start_lfc)
```

## A different way to query shared/unique genes

The various gene sets observed by these methods may also be compared
directly by an upset plot using the deseq_de_cf with
(gb|glm|rf|knn)_variable_importance...

```{r}
comp_number <- 300
de_ml_sig_list <- list(
  "DESeq2" = rownames(deseq_de_cf),
  "knn" = rownames(head(knn_variables, n = comp_number)),
  "rf" = rownames(head(rf_variables, n = comp_number)),
  "glm" = rownames(head(glm_variables, n = comp_number)),
  "gb" = rownames(head(gb_variables, n = comp_number)))
important_genes_upset_input <- UpSetR::fromList(de_ml_sig_list)
important_genes_upset <- UpSetR::upset(important_genes_upset_input)
print(important_genes_upset)

important_genes_overlap <- overlap_groups(de_ml_sig_list)
shared_gene_ids <- overlap_geneids(important_genes_overlap, "DESeq2:knn")
shared_annotations <- fData(input_data)[as.character(shared_gene_ids), ]

important_genes_overlap <- overlap_groups(de_ml_sig_list)
shared_gene_ids <- overlap_geneids(important_genes_overlap, "DESeq2:rf")
shared_annotations <- fData(input_data)[as.character(shared_gene_ids), ]

important_genes_overlap <- overlap_groups(de_ml_sig_list)
shared_gene_ids <- overlap_geneids(important_genes_overlap, "DESeq2:gb")
shared_annotations <- fData(input_data)[as.character(shared_gene_ids), ]

important_genes_overlap <- overlap_groups(de_ml_sig_list)
shared_gene_ids <- overlap_geneids(important_genes_overlap, "DESeq2:glm")
shared_annotations <- fData(input_data)[as.character(shared_gene_ids), ]
```

# Collate shared genes by method and write them

Over the course of this document I recorded the limited agreement
between the DESeq result and the ML classifier in the data structure
'comparison_lfc'  Lets write that out to an xlsx file...

```{r}
de_ml_xlsx <- glue("excel/tc_compare_de_ml_top3kvariant-v{ver}.xlsx")
xlsx <- init_xlsx(de_ml_xlsx)
wb <- xlsx[["wb"]]
excel_basename <- xlsx[["basename"]]
written <- write_xlsx(data = comparison_lfc[["knn"]], wb = wb, sheet = "knn_vs_deseq")
written <- write_xlsx(data = comparison_lfc[["rf"]], wb = wb, sheet = "rf_vs_deseq")
written <- write_xlsx(data = comparison_lfc[["glm"]], wb = wb, sheet = "glm_vs_deseq")
written <- write_xlsx(data = comparison_lfc[["gb"]], wb = wb, sheet = "gb_vs_deseq")
excel_ret <- openxlsx::saveWorkbook(wb, de_ml_xlsx, overwrite = TRUE)
```

I realized something important in this block: at no point in any of my
analyses do I really record the classifications, I just record their
(dis)concordance with our known annotations.  In the following block I
iterate over multiple rounds of the various classifiers, let us modify
that function to record this information in some useful fashion.

# Performing multiple train/test rounds in one call

Doing all of the above for each method and collecting the results is
super fiddly, so I wrote a little function to try to make that easier.

WARNING: Turning on the following block takes ~ 2 days on our pretty
nice server.  Your mileage may vary.

## Setting up input data for multiple ML iterations

There are four versions of the data that Maria Adelaida and Najib are
interested in:

* Tumaco+Cali all visits
* Tumaco+Cali visit 1
* Tumaco all visits
* Tumaco visit 1

Ergo, let us set up an appropriate naming convention for them to make
these iterative runs more coherent.  I will continue using 'tc' and
't' for the clinics as a prefix and follow it with 'vall' vs. 'v1'.

```{r}
tc_vall_input <- subset_expt(tc_clinical, subset = "batch!='biopsy'")
t_vall_input <- subset_expt(tc_vall_input, subset = "clinic=='tumaco'")
tc_v1_input <- subset_expt(tc_vall_input, subset = "visitbipart=='v1'")
t_v1_input <- subset_expt(t_vall_input, subset = "visitbipart=='v1'")

tc_vall_input <- normalize_expt(tc_vall_input, transform = "log2", convert = "cpm")
t_vall_input <- normalize_expt(t_vall_input, transform = "log2", convert = "cpm")
tc_v1_input <- normalize_expt(tc_v1_input, transform = "log2", convert = "cpm")
t_v1_input <- normalize_expt(t_v1_input, transform = "log2", convert = "cpm")

tc_vall_outcome_factor <- as.factor(as.character(pData(tc_vall_input)[[ref_col]]))
t_vall_outcome_factor <- as.factor(as.character(pData(t_vall_input)[[ref_col]]))
tc_v1_outcome_factor <- as.factor(as.character(pData(tc_v1_input)[[ref_col]]))
t_v1_outcome_factor <- as.factor(as.character(pData(t_v1_input)[[ref_col]]))
```

## Perform normalization/filtering operations as per the exploratory document

In the following block I will use the same normalizations and filters
as above on each of the four inputs.

```{r}
tc_vall_norm <- normalize_expt(tc_vall_input, filter = "cv", cv_min = 0.1)
tc_v1_norm <- normalize_expt(tc_v1_input, filter = "cv", cv_min = 0.1)
t_vall_norm <- normalize_expt(t_vall_input, filter = "cv", cv_min = 0.1)
t_v1_norm <- normalize_expt(t_v1_input, filter = "cv", cv_min = 0.1)

tc_vall_texprs <- t(exprs(tc_vall_norm))
tc_v1_texprs <- t(exprs(tc_v1_norm))
t_vall_texprs <- t(exprs(t_vall_norm))
t_v1_texprs <- t(exprs(t_v1_norm))

sd_genes <- 3000
tc_vall_stdev <- apply(tc_vall_texprs, 2, sd)
tc_vall_pred <- order(tc_vall_stdev, decreasing = TRUE)[1:sd_genes]
tc_vall_texprs <- tc_vall_texprs[, tc_vall_pred]
tc_v1_stdev <- apply(tc_v1_texprs, 2, sd)
tc_v1_pred <- order(tc_v1_stdev, decreasing = TRUE)[1:sd_genes]
tc_v1_texprs <- tc_v1_texprs[, tc_v1_pred]
t_vall_stdev <- apply(t_vall_texprs, 2, sd)
t_vall_pred <- order(t_vall_stdev, decreasing = TRUE)[1:sd_genes]
t_vall_texprs <- t_vall_texprs[, t_vall_pred]
t_v1_stdev <- apply(t_v1_texprs, 2, sd)
t_v1_pred <- order(t_v1_stdev, decreasing = TRUE)[1:sd_genes]
t_v1_texprs <- t_v1_texprs[, t_v1_pred]

tc_vall_preproc <- preProcess(tc_vall_texprs, method = "center")
tc_vall_texprs <- predict(tc_vall_preproc, tc_vall_texprs)
tc_v1_preproc <- preProcess(tc_v1_texprs, method = "center")
tc_v1_texprs <- predict(tc_v1_preproc, tc_v1_texprs)
t_vall_preproc <- preProcess(t_vall_texprs, method = "center")
t_vall_texprs <- predict(t_vall_preproc, t_vall_texprs)
t_v1_preproc <- preProcess(t_v1_texprs, method = "center")
t_v1_texprs <- predict(t_v1_preproc, t_v1_texprs)

tc_vall_preproc <- preProcess(tc_vall_texprs, method = "corr", cutoff = 0.95)
tc_vall_texprs <- predict(tc_vall_preproc, tc_vall_texprs)
tc_v1_preproc <- preProcess(tc_v1_texprs, method = "corr", cutoff = 0.95)
tc_v1_texprs <- predict(tc_v1_preproc, tc_v1_texprs)
t_vall_preproc <- preProcess(t_vall_texprs, method = "corr", cutoff = 0.95)
t_vall_texprs <- predict(t_vall_preproc, t_vall_texprs)
t_v1_preproc <- preProcess(t_v1_texprs, method = "corr", cutoff = 0.95)
t_v1_texprs <- predict(t_v1_preproc, t_v1_texprs)

chosen_factors <- c("finaloutcome", "donor", "persistence", "visitnumber",
                    "selectionmethod", "typeofcells", "time")
tc_vall_meta <- pData(tc_vall_norm)[, chosen_factors]
tc_v1_meta <- pData(tc_v1_norm)[, chosen_factors]
t_vall_meta <- pData(t_vall_norm)[, chosen_factors]
t_v1_meta <- pData(t_v1_norm)[, chosen_factors]

ref_col <- "finaloutcome"
tc_vall_outcome <- as.factor(as.character(pData(tc_vall_norm)[[ref_col]]))
tc_v1_outcome <- as.factor(as.character(pData(tc_v1_norm)[[ref_col]]))
t_vall_outcome <- as.factor(as.character(pData(t_vall_norm)[[ref_col]]))
t_v1_outcome <- as.factor(as.character(pData(t_v1_norm)[[ref_col]]))

tc_vall_ml_df <- cbind.data.frame(tc_vall_outcome, as.data.frame(tc_vall_texprs))
tc_vall_ml_df[[ref_col]] <- as.factor(tc_vall_ml_df[["tc_vall_outcome"]])
tc_v1_ml_df <- cbind.data.frame(tc_v1_outcome, as.data.frame(tc_v1_texprs))
tc_v1_ml_df[[ref_col]] <- as.factor(tc_v1_ml_df[["tc_v1_outcome"]])
t_vall_ml_df <- cbind.data.frame(t_vall_outcome, as.data.frame(t_vall_texprs))
t_vall_ml_df[[ref_col]] <- as.factor(t_vall_ml_df[["t_vall_outcome"]])
t_v1_ml_df <- cbind.data.frame(t_v1_outcome, as.data.frame(t_v1_texprs))
t_v1_ml_df[[ref_col]] <- as.factor(t_v1_ml_df[["t_v1_outcome"]])
```

## Iterate Tumaco+Cali all visits

```{r, eval=FALSE}
tc_vall_summary_xlsx <- glue("excel/tc_vall_ml_summary-v{ver}.xlsx")
tc_vall_knn <- classify_n_times(tc_vall_texprs, tc_vall_meta,
                                outcome_column = ref_col,
                                method = "knn", sampler = "cv")
written <- write_classifier_summary(tc_vall_knn, excel = tc_vall_summary_xlsx)
tc_vall_gb <- classify_n_times(tc_vall_texprs, tc_vall_meta,
                               outcome_column = ref_col,
                               method = "xgbTree", sampler = "cv")
written <- write_classifier_summary(tc_vall_gb, excel = written[["wb"]])
tc_vall_glm <- classify_n_times(tc_vall_texprs, tc_vall_meta,
                                outcome_column = ref_col,
                                method = "glmnet", sampler = "cv")
written <- write_classifier_summary(tc_vall_glm, excel = written[["wb"]])
tc_vall_rf <- classify_n_times(tc_vall_texprs, tc_vall_meta,
                               outcome_column = ref_col,
                               method = "ranger", sampler = "cv")
written <- write_classifier_summary(tc_vall_rf, excel = written[["wb"]])
openxlsx::saveWorkbook(written[["wb"]], file = tc_vall_summary_xlsx)
```

## Iterate Tumaco+Cali Visit 1

```{r, eval=FALSE}
tc_v1_summary_xlsx <- glue("excel/tc_v1_ml_summary-v{ver}.xlsx")
tc_v1_knn <- classify_n_times(tc_v1_texprs, tc_v1_meta,
                                outcome_column = ref_col,
                                method = "knn", sampler = "cv")
written <- write_classifier_summary(tc_v1_knn, excel = tc_v1_summary_xlsx)
tc_v1_gb <- classify_n_times(tc_v1_texprs, tc_v1_meta,
                               outcome_column = ref_col,
                               method = "xgbTree", sampler = "cv")
written <- write_classifier_summary(tc_v1_gb, excel = written[["wb"]])
tc_v1_glm <- classify_n_times(tc_v1_texprs, tc_v1_meta,
                                outcome_column = ref_col,
                                method = "glmnet", sampler = "cv")
written <- write_classifier_summary(tc_v1_glm, excel = written[["wb"]])
tc_v1_rf <- classify_n_times(tc_v1_texprs, tc_v1_meta,
                               outcome_column = ref_col,
                               method = "ranger", sampler = "cv")
written <- write_classifier_summary(tc_v1_rf, excel = written[["wb"]])
openxlsx::saveWorkbook(written[["wb"]], file = tc_v1_summary_xlsx)
```

## Tumaco all visits

```{r, eval=FALSE}
t_vall_summary_xlsx <- glue("excel/t_vall_ml_summary-v{ver}.xlsx")
t_vall_knn <- classify_n_times(t_vall_texprs, t_vall_meta,
                                outcome_column = ref_col,
                                method = "knn", sampler = "cv")
written <- write_classifier_summary(t_vall_knn, excel = t_vall_summary_xlsx)
t_vall_gb <- classify_n_times(t_vall_texprs, t_vall_meta,
                               outcome_column = ref_col,
                               method = "xgbTree", sampler = "cv")
written <- write_classifier_summary(t_vall_gb, excel = written[["wb"]])
t_vall_glm <- classify_n_times(t_vall_texprs, t_vall_meta,
                                outcome_column = ref_col,
                                method = "glmnet", sampler = "cv")
written <- write_classifier_summary(t_vall_glm, excel = written[["wb"]])
t_vall_rf <- classify_n_times(t_vall_texprs, t_vall_meta,
                               outcome_column = ref_col,
                               method = "ranger", sampler = "cv")
written <- write_classifier_summary(t_vall_rf, excel = written[["wb"]])
openxlsx::saveWorkbook(written[["wb"]], file = t_vall_summary_xlsx)
```

## Tumaco Visit 1

```{r, eval=FALSE}
t_v1_summary_xlsx <- glue("excel/t_v1_ml_summary-v{ver}.xlsx")
t_v1_knn <- classify_n_times(t_v1_texprs, t_v1_meta,
                                outcome_column = ref_col,
                                method = "knn", sampler = "cv")
written <- write_classifier_summary(t_v1_knn, excel = t_v1_summary_xlsx)
t_v1_gb <- classify_n_times(t_v1_texprs, t_v1_meta,
                            outcome_column = ref_col,
                               method = "xgbTree", sampler = "cv")
written <- write_classifier_summary(t_v1_gb, excel = written[["wb"]])
t_v1_glm <- classify_n_times(t_v1_texprs, t_v1_meta,
                                outcome_column = ref_col,
                                method = "glmnet", sampler = "cv")
written <- write_classifier_summary(t_v1_glm, excel = written[["wb"]])
t_v1_rf <- classify_n_times(t_v1_texprs, t_v1_meta,
                               outcome_column = ref_col,
                               method = "ranger", sampler = "cv")
written <- write_classifier_summary(t_v1_rf, excel = written[["wb"]])
openxlsx::saveWorkbook(written[["wb"]], file = t_v1_summary_xlsx)
```

# Bibliography
