---
title: "TMRC3 ML Classification of outcome, Tumaco samples: `r Sys.getenv('VERSION')`"
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
output:
 html_document:
  code_download: true
  code_folding: show
  fig_caption: true
  fig_height: 7
  fig_width: 7
  highlight: tango
  keep_md: false
  mode: selfcontained
  number_sections: true
  self_contained: true
  theme: readable
  toc: true
  toc_float:
   collapsed: false
   smooth_scroll: false
---

<style>
  body .main-container {
    max-width: 1600px;
  }
</style>

```{r, include=FALSE}
library(hpgltools)
library(caret)
library(dplyr)
library(pROC)
library(DALEX)
library(glmnet)
library(glue)
library(kernlab)
library(ranger)
library(xgboost)
library(ggstatsplot)
knitr::opts_knit$set(
  progress = TRUE, verbose = TRUE, width = 90, echo = TRUE)
knitr::opts_chunk$set(
  error = TRUE, fig.width = 8, fig.height = 8, fig.retina = 2,
  fig.pos = "t", fig.align = "center", dpi = if (knitr::is_latex_output()) 72 else 300,
  out.width = "100%", dev = "png",
  dev.args = list(png = list(type = "cairo-png")))
old_options <- options(digits = 4,
                       stringsAsFactors = FALSE,
                       knitr.duplicate.label = "allow")
ggplot2::theme_set(ggplot2::theme_bw(base_size = 12))
ver <- Sys.getenv("VERSION")
rundate <- format(Sys.Date(), format = "%Y%m%d")
previous_file <- ""
rmd_file <- "06classifier.Rmd"
savefile <- gsub(pattern = "\\.Rmd", replace = "\\.rda\\.xz", x = rmd_file)
loaded <- load(file = glue("rda/tmrc3_data_structures-v{ver}.rda"))
```

# Introduction

I had some success in classifying the TMRC2 samples by strain via ML
and want to try something more difficult.  Thus I will use the
normalized gene expression data and try classifying it by cure/fail.

# Starter data

In the strain classifier I used normalized variants.  I am thinking to
use normalized expression here and therefore explicitly limit myself
to ~ 20k variables (significantly down from the 1.6M).

In addition, caret expects the data as (rows == samples) and
(columns == variables) where each element is one observation.
Thus we will need to transpose the expression matrix.

```{r}
input_data <- subset_expt(t_clinical, subset="batch!='biopsy'")
t_norm <- normalize_expt(input_data, transform = "log2", convert = "cpm")

ref_col <- "finaloutcome"
outcome_factor <- as.factor(as.character(pData(input_data)[[ref_col]]))
comparison_n <- 200
```

# Changelog

*202309: Adding a version of this with only the Tumaco data.  I am a
pretty big non-fan of doing this; so I am going to do so in the most
cursory fashion possible by just copying this document to another copy with
a suffix of 'tumaco' and changing the initial datastructure to the Tumaco-only
dataset.

# Filtering

The ML text I am reading provide some neat examples for how one might
filter the data to make it more suitable for model creation.

## Near zero variance, or genefilter's cv

The first filter I was introduced to is quite familiar from our
sequencing data, the removal of features with near-zero-variance.
Indeed, I am pretty certain that normalize_expt() could do this
equivalently and significantly faster than caret::preProcess().

```{r}
system.time({
  equivalent <- normalize_expt(t_norm, filter = "cv", cv_min = 0.1)
})
dim(exprs(equivalent))

## Given this large amount of data, this step is slow, taking > 10 minutes.
## Yeah seriously, the following three lines get 16,723 genes in 10 minutes while
## the normalize_expt() call above gets 16,749 genes in 2.4 seconds.
#system.time({
#  nzv <- preProcess(texprs, method="nzv", uniqueCut=15)
#  nzv_texprs <- predict(nzv, texprs)
#  dim(nzv_texprs)
#}
nzv_texprs <- t(exprs(equivalent))
```

## Filtering to the highest standard deviation variables

I think I am a bit confused by this filter, one would think that the
nzv filter above, if applied correctly, should give you exactly this.

For the moment, I am excluding the following block in order to see how
much time/memory keeping these variables costs.  If I recall properly,
the model of the top-2k variant positions cost ~ 1-4G of memory.  I
hope that this scales linearly, but I am thinking it might not.

```{r, eval=FALSE}
standard_devs <- apply(nzv_texprs, 2, sd)
top_predictors <- order(standard_devs, decreasing = TRUE)[1:3000]
nzv_texprs <- nzv_texprs[, top_predictors]
```

## Center the data

I think centering may not be needed for this data, but here is how:

```{r}
nzv_center <- preProcess(nzv_texprs, method = "center")
nzv_texprs <- predict(nzv_center, nzv_texprs)
```

## Drop correlated

This is a filter which does not correspond to any of those we use in
sequencing data because genes which are highly correlated are
likely to be of immediate interest.

In the same fashion, I want to leave this off because later
applications of this model will include low coverage samples which may
not have every variant represented.

```{r}
## This step takes a while...
system.time({
  nzv_correlated <- preProcess(nzv_texprs, method = "corr", cutoff = 0.95)
  nzv_uncorr <- predict(nzv_correlated, nzv_texprs)
})
dim(nzv_uncorr)
```

# Merge the appropriate metadata

There are a few metadata factors which might prove of interest for
classification.  The most obvious are of course outcome, clinic,
donor, visit, celltype.  I am, for the moment, only likely to focus on
outcome.  AFAICT I can only include one of these at a time in the
data, which is a shame.

```{r}
interesting_meta <- pData(input_data)[, c("finaloutcome", "donor", "persistence",
                                          "visitnumber", "selectionmethod",
                                          "typeofcells", "time", "clinic")]

ml_df <- as.data.frame(cbind(outcome_factor, as.data.frame(nzv_uncorr)))
ml_df[["outcome_factor"]] <- as.factor(ml_df[["outcome_factor"]])
dim(ml_df)
```

# Split the data into training/testing

caret provides nice functionality for splitting up the data.  I
suspect there are many more fun knobs I can play with for instances
where I need to exclude some levels of a factor and such.  In this
case I just want to split by outcome.

## Via data splitting

```{r}
ml_df <- as.data.frame(cbind(outcome_factor, as.data.frame(nzv_uncorr)))

datasets <- create_partitions(nzv_uncorr, interesting_meta,
                              outcome_factor = outcome_factor)
```

## Via sampling

There are a few likely sampling methods: cross-validation,
bootstrapping, and jackknifing.  I will try those out later.

# Try out training and prediction methods

My goals from here on will be to get the beginnings of a sense of the
various methods I can use to create the models from the training data
and predict the outcome on the test data.  I am hoping also to pick up
some idea of what the various arguments mean while I am at it.

## Try out KNN

k-nearest neighbors is somewhat similar to a kmeans estimate.  Thus
the primary argument is 'k'

### Model creation and performance

```{r}
split <- 1
train_all <- datasets[["trainers"]][[split]]
train_df <- datasets[["trainers_stripped"]][[split]]
train_idx <- datasets[["train_idx"]][[split]]
train_outcomes <- datasets[["trainer_outcomes"]][[split]]
test_df <- datasets[["testers"]][[split]]
test_idx <- datasets[["test_idx"]][[split]]
test_outcomes <- datasets[["tester_outcomes"]][[split]]

knn_fit <- knn3(x = train_df,
                y = train_outcomes,
                k = 3)
knn_predict_trained <- predict(knn_fit, train_df, type = "prob")

knn_train_evaluated <- self_evaluate_model(knn_predict_trained, datasets,
                                           which = split, type = "train")
knn_train_evaluated
```

As the confusion matrix shows, this failed for a few samples.  Perhaps
let us change k and see if it improves.

Here is a table of fase positives/negatives for a few values of 'k',
in this context a false positive is calling a known cure as a failure
and false negative is calling a known failure as a cure.

|---|---|---|
|k  |fp |fn |
|2  |0  |8  |
|3  |5  |5  |
|4  |8  |9  |
|5  |11 |7  |
|6  |15 |8  |

Note: this depends on the luck of rand(), so the above numbers shift
moderately from one run to the next.  Thus I think I will just use 2
or 3.

```{r}
knn_fit2 <- knn3(x = train_df,
                y = train_outcomes,
                k = 5)
knn_predict_trained2 <- predict(knn_fit2, train_df, type = "prob")

knn_train_evaluated2 <- self_evaluate_model(knn_predict_trained2, datasets,
                                            which = split, type = "train")
knn_train_evaluated2
```

### Predict the rest of the data with this model.

```{r}
knn_predict_test <- predict(knn_fit, test_df)

knn_test_evaluated <- self_evaluate_model(knn_predict_test, datasets,
                                     which = split, type = "test")
knn_test_evaluated

knn_predict_test2 <- predict(knn_fit2, test_df)
knn_test_evaluated2 <- self_evaluate_model(knn_predict_test2, datasets,
                                           which = split, type = "test")
knn_test_evaluated2
```

## Perform cross-validation to estimate k

The cross validation method of repeated sampling the data is all done
within the train() function.  With that in mind, here it is operating
with the knn method.

### CV with knn

When train() is called with the trControl and tuneGrid, we can control
how the knn training is repeated, in this case it will iterate over k
from 1 to 10.

This currently fails due to a stack overflow...

```{r, eval=FALSE}
cv_control <- trainControl(method = "cv", number = 10)

knn_train_fit <- train(outcome_factor ~ ., data = train_df,
                       method = "knn",
                       trControl = cv_control,
                       tuneGrid = data.frame(k = 1:10))
knn_train_fit[["bestTune"]]

plot(x = 1:10, 1 - knn_train_fit$results[, 2], pch = 19,
     ylab = "prediction error", xlab = "k")
lines(loess.smooth(x = 1:10, 1 - knn_train_fit$results[, 2],degree = 2),
      col = "#CC0000")
```

### Bootstrap with knn

```{r}
boot_control <- trainControl(method = "boot", number = 20,
                             returnResamp = "all")

knn_train_fit <- train(outcome ~ ., data = train_all,
                       method = "knn",
                       trControl = boot_control,
                       tuneGrid = data.frame(k = 1:10))
knn_train_fit[["bestTune"]]

plot(x = 1:10, 1 - knn_train_fit$results[, 2], pch = 19,
     ylab = "prediction error", xlab = "k")
lines(loess.smooth(x = 1:10, 1 - knn_train_fit$results[, 2],degree = 2),
      col = "#CC0000")
```

### Explain the important variables

In this instance we will search for genes which were important for the
model's creation.

The DALEX package provides a function: feature_importance() which
seeks to use a series of other methods to extract (in this case,
genes) features which do a good job of explaining the result produced
by the model.  In the case of this dataset, which has thousands of
features, this does not appear to end well.

```{r, eval=FALSE}
explainer_knn <- DALEX::explain(knn_fit, label = "knn",
                                data = train_df,
                                y = as.numeric(train_outcomes))

## AFAICT the following will take forever unless we drastically reduce the complexity of the model.
## yeah, I let it run for a week.
## features <- feature_importance(explainer_knn, n_sample = 50, type = "difference")
```

## Random Forest

The parameter 'mtry' is often important, if I read the text correctly
it controls how many variables to sample in each split of the tree.
Thus higher numbers should presumably make it more specific at the
risk of overfitting.

Setting min.node.size sets the minimume node size of terminal nodes in
each tree.  Each increment up speeds the algorithm.

I am going to use my boot control trainer from above and see how it goes.

```{r}
rf_train_fit <- train(outcome ~ ., data = train_all,
                method = "ranger", trControl = boot_control,
                importance = "permutation",
                tuneGrid = data.frame(
                    mtry = 200,
                    min.node.size = 1,
                    splitrule = "gini"),
                verbose = TRUE)
rf_train_fit[["finalModel"]][["prediction.error"]]

variable_importance <- varImp(rf_train_fit)
plot(variable_importance, top = 15)
rf_variables <- variable_importance[["importance"]] %>%
  arrange(desc(Overall))

rf_predict_trained <- predict(rf_train_fit, train_df)
rf_predict_evaluated <- self_evaluate_model(rf_predict_trained, datasets,
                                            which = split, type = "train")
rf_predict_evaluated
```

## Compare topn important genes to DE genes

Given that we have separated the various analyses, it will take me a
minute to figure out where I saved the relevant differential
expression analysis.  I do not actually save the various DE results to
rda files by default, instead opting to send them to xlsx files to
share.  Recall if you will, that the data that I think might be used
for the paper also does not go into the default excel directory but
instead mirrors the box organization scheme.

Thus, I think the most relevant file is:
"analyses/4_tumaco/DE_Cure_vs_Fail/All_Samples/t_cf_clinical_tables_sva-v202207.xlsx"

```{r}
input_xlsx = glue("analyses/4_tumaco/DE_Cure_vs_Fail/t_all_visitcf_tables_sva-v{ver}.xlsx")
all_de_cf <- openxlsx::readWorkbook(input_xlsx, sheet = 2, startRow = 2)
rownames(all_de_cf) <- all_de_cf[["row.names"]]
all_de_cf[["row.names"]] <- NULL
deseq_de_cf <- all_de_cf[, c("deseq_logfc", "deseq_adjp", "deseq_basemean", "deseq_lfcse")]
```

### What would be shared between DESeq2 and the ML classifier?

Presumably DESeq and the models should be responding to variance in
the data, for which I think the logFC values, p-values, mean values,
or standard errors are the most likely proxies to which I have easy
access.  So, let us pull the top/bottom n genes vis a vis each of
those categories and see what happens?

```{r}
comparison_lfc <- list()
comparison_adjp <- list()
comparison_wgcna <- list()

top_lfc <- all_de_cf %>%
  arrange(desc(deseq_logfc)) %>%
  top_n(n = comparison_n, wt = deseq_logfc)
bottom_lfc <- all_de_cf %>%
  arrange(deseq_logfc) %>%
  top_n(n = -1 * comparison_n, wt = deseq_logfc)
top_bottom_ids <- c(rownames(top_lfc), rownames(bottom_lfc))

top_ml <- rownames(head(rf_variables, n = comparison_n))
comparison <- list("de" = top_bottom_ids, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_lfc[["rf"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

top_adjp <- all_de_cf %>%
  top_n(n = -1 * comparison_n, wt = deseq_adjp)
lowest_adjp <- rownames(top_adjp)
comparison <- list("de" = lowest_adjp, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_adjp[["rf"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

top_exprs <- all_de_cf %>%
  top_n(n = comparison_n, wt = deseq_basemean)
highest_exprs <- rownames(top_exprs)
comparison <- list("de" = highest_exprs, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

tt <- merge(all_de_cf, rf_variables, by = "row.names")
rownames(tt) <- tt[["Row.names"]]
tt[["Row.names"]] <- NULL
cor.test(tt[["deseq_logfc"]], tt[["Overall"]])
```

### Compare to the WGCNA results

A couple months ago I spent a little time attempting to recapitulate
Alejandro's WGCNA results.  I think I did so by mostly copy/pasting
his work and adding some commentary and tweaking parts of it so that
it was easier for me to read/understand.  In the process, I generated
a series of modules which looked similar/identical to his.
Unfortunately, I did not add some sections to record the genes/modules
to some output files.  I am therefore going back to that now and doing
so in the hopes that I can compare those modules to the results
produced by the clasifiers.

```{r}
wgcna_result <- openxlsx::readWorkbook(glue("excel/wgcna_interesting_genes-v{ver}.xlsx"))
rownames(wgcna_result) <- wgcna_result[["row.names"]]
wgcna_result[["row.names"]] <- NULL

top_ml <- rownames(head(rf_variables, n = comparison_n))
comparison <- list("wgcna" = rownames(wgcna_result), "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_wgcna[["rf"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

#### Digression do the genes provide by varImp mean anything?

Let us take a moment and see if the top-n genes returned by varImp()
have some meaning which jumps out.  One might assume, given our extant
Differential Expression results, that the interleukin response will be
a likely candidate.

```{r}
importance_gp <- simple_gprofiler(rownames(head(rf_variables, n = comparison_n)))
importance_gp
```

### Now the random forest testers!

```{r}
rf_predict_test <- predict(rf_train_fit, test_df)

rf_predict_test_evaluated <- self_evaluate_model(rf_predict_test, datasets,
                                     which = split, type = "test")
rf_predict_test_evaluated
```

## GLM, or Logistic regression and regularization

Logistic regression is a statistical method for binary responses.
However, it is able to work with multiple classes as well.  The
general idea of this method is to find parameters which increase the
likelihood that the observed data is sampled from a statistical
distribution of interest.  The transformations and linear
regression-esque tasks performed are confusing, but once those are
performed, the task becomes setting the model's (fitting) parameters
to values which increase the probability that the statistical model
looks like the actual dataset given the training data, and that when
samples, will return values which are similar.  The most likely
statistical distributions one will want to fit are the Gaussian, in
which case we want to transform/normalize the mean/variance of our
variables so they look whatever normal distribution we are using.
Conversely, logistic regression uses a binnomial distribution (like
our raw sequencing data!) but which is from 0-1.

### Using a single gene

Let us take the most important gene observed in one of our previous
training sets: ENSG00000248405 PRR5-ARHGAP8

```{r}
gene_id <- "ENSG00000248405"
single_fit <- train(
    outcome ~ ENSG00000248405, data = train_all,
    method = "glm", family = "binomial", trControl = trainControl("none"))

tt <- data.frame("ENSG00000248405" = seq(min(train_df[[gene_id]]),
                                         max(train_df[[gene_id]]), len = 100))
## predict probabilities for the simulated data
tt$subtype = predict(single_fit, newdata = tt, type="prob")[, 1]
## plot the sigmoid curve and the training data
plot(ifelse(outcome == "cure", 1, 0) ~ ENSG00000248405,
     data = train_all, col = "red4",
     ylab = "CF as 0 or 1", xlab = "favorite gene expression")
lines(subtype ~ ENSG00000248405, tt, col = "green4", lwd = 2)

plot_df <- train_all[, c("outcome", "ENSG00000248405")]
ggbetweenstats(plot_df, "outcome", "ENSG00000248405")
```

Having tried with 1 gene, let us extend this to all genes.  In my
first try of this, it took a long time.

```{r}
glm_train_fit <- train(outcome ~ ., data = train_all,
                 trControl = boot_control,
                 method = "glm", family = "binomial")
```

## Compare GLM and WGCNA/DE

```{r}
glm_variable_importance <- varImp(glm_train_fit)
## Oh, this only produces 100 entries -- so me getting the top 400 is silly.
glm_variables <- glm_variable_importance[["importance"]] %>%
  arrange(desc(Overall))
plot(glm_variable_importance, top = 15)
simple_gprofiler(rownames(head(glm_variables, n = comparison_n)))

top_glm <- rownames(glm_variables)
comparison <- list("de" = top_bottom_ids, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
comparison_lfc[["glm"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = lowest_adjp, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
comparison_adjp[["glm"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = highest_exprs, "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("wgcna" = rownames(wgcna_result), "ml" = top_glm)
comparison_venn <- Vennerable::Venn(comparison)
comparison_wgcna[["glm"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

```{r}
##rf_method <- trainControl(method = "ranger", number = 10, verbose = TRUE)
## train_method <- trainControl(method = "cv", number = 10)
glm_fit <- train(outcome ~ ., data = train_all, method = "glmnet",
                 trControl = boot_control, importance = "permutation",
                 tuneGrid = data.frame(
                   alpha = 0.5,
                   lambda = seq(0.1, 0.7, 0.05)),
                 verbose = TRUE)
glm_fit

glm_predict_trained <- predict(glm_fit, train_df)

glm_train_eval <- self_evaluate_model(glm_predict_trained, datasets,
                                      which = split, type = "train")
glm_train_eval
```

### Now the GLM testers!

```{r}
glm_predict_test <- predict(glm_fit, test_df)

glm_fit_eval_test <- self_evaluate_model(glm_predict_test, datasets,
                                         which = split, type = "test")
glm_fit_eval_test
```

### Compare again vs DE/WGCNA

```{r}
variable_importance <- varImp(glm_fit)
plot(variable_importance, top = 15)

top_ml <- rownames(head(variable_importance$importance, n = comparison_n))
comparison <- list("de" = top_bottom_ids, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = lowest_adjp, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = highest_exprs, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
## No overlap
##fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

top_ml <- rownames(head(variable_importance$importance, n = comparison_n))
comparison <- list("wgcna" = rownames(wgcna_result), "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

## Gradient Booster

```{r}
##rf_method <- trainControl(method = "ranger", number = 10, verbose = TRUE)
train_method <- trainControl(method = "cv", number = 10)

gb_fit <- train(outcome ~ ., data = train_all,
                method = "xgbTree", trControl = train_method,
                tuneGrid = data.frame(
                    nrounds = 200,
                    eta = c(0.05, 0.1, 0.3),
                    max_depth = 4,
                    gamma = 0,
                    colsample_bytree = 1,
                    subsample = 0.5,
                    min_child_weight = 1),
                verbose = TRUE)

gb_predict_trained <- predict(gb_fit, train_df)
gb_predict_trained

gb_train_eval <- self_evaluate_model(gb_predict_trained, datasets,
                                     which = split, type = "train")
gb_train_eval
```

### Now the GB testers!

```{r}
gb_predict_test <- predict(gb_fit, test_df)

gb_predict_test_evaluated <- self_evaluate_model(gb_predict_test, datasets,
                                                 which = split, type = "test")
gb_predict_test_evaluated
```

```{r}
variable_importance <- varImp(gb_fit)
plot(variable_importance, top = 15)

top_ml <- rownames(head(variable_importance$importance, n = comparison_n))

comparison <- list("de" = top_bottom_ids, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_lfc[["gb"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = lowest_adjp, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_adjp[["gb"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

comparison <- list("de" = highest_exprs, "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")

top_ml <- rownames(head(variable_importance$importance, n = comparison_n))
comparison <- list("wgcna" = rownames(wgcna_result), "ml" = top_ml)
comparison_venn <- Vennerable::Venn(comparison)
comparison_wgcna[["gb"]] <- fData(c_monocytes)[comparison_venn@IntersectionSets["11"][[1]], ]
Vennerable::plot(comparison_venn, doWeights = FALSE, type = "circles")
```

# Shared importance

```{r}
upset_lfc <- list()
upset_adjp <- list()
upset_wgcna <- list()
for (d in 1:length(comparison_lfc)) {
  name <- names(comparison_lfc)[d]
  upset_lfc[[name]] <- rownames(comparison_lfc[[name]])
  upset_adjp[[name]] <- rownames(comparison_adjp[[name]])
  upset_wgcna[[name]] <- rownames(comparison_wgcna[[name]])
}

start_lfc <- UpSetR::fromList(upset_lfc)
UpSetR::upset(start_lfc)

start_adjp <- UpSetR::fromList(upset_adjp)
UpSetR::upset(start_adjp)

start_wgcna <- UpSetR::fromList(upset_wgcna)
UpSetR::upset(start_wgcna)
```
