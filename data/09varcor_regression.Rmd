---
title: "TMRC3 `r Sys.getenv('VERSION')`: Correlation among metadata variables and regression analyses."
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
bibliography: atb.bib
runtime: shiny
output:
  html_document:
    code_download: true
    code_folding: show
    fig_caption: true
    fig_height: 7
    fig_width: 7
    highlight: zenburn
    keep_md: false
    mode: selfcontained
    number_sections: true
    self_contained: true
    theme: readable
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
  rmdformats::readthedown:
    code_download: true
    code_folding: show
    df_print: paged
    fig_caption: true
    fig_height: 7
    fig_width: 7
    highlight: zenburn
    width: 300
    keep_md: false
    mode: selfcontained
    toc_float: true
  BiocStyle::html_document:
    code_download: true
    code_folding: show
    fig_caption: true
    fig_height: 7
    fig_width: 7
    highlight: zenburn
    keep_md: false
    mode: selfcontained
    toc_float: true
---

<style type="text/css">
body, td {
  font-size: 16px;
}
code.r{
  font-size: 16px;
}
pre {
  font-size: 16px
}
body .main-container {
  max-width: 1600px;
}
</style>

\usepackage{booktabs}
\usepackage{amsthm}
\newcommand{\ci}{\perp\!\!\!\perp}

```{r options, include=FALSE}
library(dplyr)
library(ggplot2)
library(ggstatsplot)
library(hpgltools)
library(lares)
library(patchwork)
library(reticulate)
library(rpart)
library(rpart.plot)
library(tidyr)

knitr::opts_knit$set(
  progress = TRUE, verbose = TRUE, width = 90, echo = TRUE)
knitr::opts_chunk$set(
  error = TRUE, fig.width = 8, fig.height = 8, fig.retina = 2,
  out.width = "100%", dev = "png",
  dev.args = list(png = list(type = "cairo-png")))
old_options <- options(digits = 4, stringsAsFactors = FALSE, knitr.duplicate.label = "allow")
ggplot2::theme_set(ggplot2::theme_bw(base_size = 12))
ver <- Sys.getenv("VERSION")
previous_file <- ""
rundate <- format(Sys.Date(), format = "%Y%m%d")

##tmp <- sm(loadme(filename=paste0(gsub(pattern="\\.Rmd", replace="", x=previous_file), "-v", ver, ".rda.xz")))
rmd_file <- "07varcor_regression.Rmd"
savefile <- gsub(pattern = "\\.Rmd", replace = "\\.rda\\.xz", x = rmd_file)
## Oh man I hope that file is not too monstrous
loaded <- load(file = glue("rda/tmrc3_data_structures-v{ver}.rda"))
```

# Important Note

Given that all of the figures/tables from here which are actually used
have been pretty heavily modified and copied/moved to other documents,
this document is now pretty much purely exploratory.

# Changelog

* 202408: Copied/Modified all of the material which is used for
  figures/tables/etc from here into other notebooks in order to
  integrate the analyses into the appropriate sections (primarily
  02visualization).
* 202406: Explicitly set text from me (atb) as dark green so that I am
  no longer so thoroughly polluting Theresa's work.

# Introduction

<span style="color:#006400">
This document is explicitly taking some of the work from Theresa's
notebooks and adding it to the singularity container.  All the good
ideas are likely hers while the mistakes are certainly my own.  This
is a composite of her tmrc3_ml and tmrc_sample_estimation
notebooks. In both cases, she loaded the initial data in a fashion
nearly identically to how I am in these worksheets, as a result I
strongly suspect that I can simply copy/paste her blocks of text and
code, run them, and get nearly identical results.  There is a decent
chance that I will thus scab some of the ideas and stick them into
hpgltools so that I may use them a bit more easily in other analyses.
I will make notes for anything which I repurpose in this fashion;
Theresa is already an author on hpgltools, so I presume this is not a
problem, but just in case I will text her tomorrow and ask. In her
notebook, she started off by plotting essentially the same PCA that I
did in my 02sample_estimation worksheet in order to ensure that we
were working from similar/identical starting points.

She then extracted the eosinophil, neutrophil, and monocyte samples
from visit 1.  Happily, the data_structures.rda we loaded at the
beginning of this document has them in it, so I need not repeat that.

She followed that with calls to all_pairwise() for each v1 celltype.
I happily also have versions of this in a few places, most similarly
(identically?) in the H3 section titled 'Individual visits, Monocytes'
and the sections following it.  The resulting data structures which
match Theresa's are thus named 't_cf_monocyte_v1_de_sva' and other
names following the conventions laid out in that document.
Unfortunately, I neglected to save the data structures of these de
analyses, so I am thinking I will add that to that and see if it
results in an excessively large rda; if so I will separately save just
those data structures which are relevant to Theresa's document.

With the above in mind, the following is just me copy/pasting
Theresa's notebook.  Once the container and data structures regenerate
tonight, I will come back and add some more context.
</span>

## The DEs of cure vs failure for each independent cell line only for V1

This is from the email Ade sent 9/6 about the tasks we discussed when
she visited.

I will do a DE for each of the cell lines for only the subsetted V1
samples. So first, I subset for only visit 1 samples, and separate by
cell type: neutrophils, eosinophils, monocytes.

# Sample Traits

In this section, I’ll start to look at the sample traits sent by Ade.

```{r}
traits <- readxl::read_xlsx("sample_sheets/tmrc3_demographicsv2.xlsx")
```

## plot new lesions by visit number

<span style="color:#006400">
I will make some small changes to Theresa's work here, primarily
formatting and changes to make the data properly load.  E.g. we
changed the demographics input file to strip out columns which might
make it possible to identify a person.  I also have an aversion to
creating a subset index and using it in the same line (I screw that
operation up so often that I pretty much always want a chance to look
at the index before trusting that I am doing the right thing); so I will
likely split those up.
</span>

### Set up a valid lesion dataframe

The data as encoded in the demographics file follows a series of
conventions which are explicitly invalid in the context of R.  Thus we
need to do a little wrangling to use it.

```{r}
lesion_columns <- c("Patient_ID", "Therapeutic_Outcome_Final", "Total_Area_Lesion",
                    "V2_Total_Area_Lesion",  "V3_Total_Area_Lesion", "V4_Total_Area_Lesion",
                    "V5_Total_Area_Lesion")
lesion_areas <- as.data.frame(traits[, lesion_columns])

colnames(lesion_areas) <- c("Patient_ID", "Cure/Fail", "Lesion_Area_1",  "V2", "V3", "V4", "V5")

v1_lesion_invalid_idx <- lesion_areas[["Lesion_Area_1"]] == 999.0 &
  !is.na(lesion_areas[["Lesion_Area_1"]])
lesion_areas[v1_lesion_invalid_idx, "Lesion_Area_1"] <- NA

v2_lesion_invalid_idx <- lesion_areas[["V2"]] == 999.0 &
  !is.na(lesion_areas[["V2"]])
lesion_areas[v2_lesion_invalid_idx, "V2"] <- NA

v3_lesion_invalid_idx <- lesion_areas[["V3"]] == 999.0 &
  !is.na(lesion_areas[["V3"]])
lesion_areas[v3_lesion_invalid_idx, "V3"] <- NA

v4_lesion_invalid_idx <- lesion_areas[["V4"]] == 999.0 &
  !is.na(lesion_areas[["V4"]])
lesion_areas[v4_lesion_invalid_idx, "V4"] <- NA

v5_lesion_invalid_idx <- lesion_areas[["V5"]] == 999.0 &
  !is.na(lesion_areas[["V5"]])
lesion_areas[v5_lesion_invalid_idx, "V5"] <- NA
```

### Repeat for the ulcer sizes

Ibid.

```{r}
ulcer_columns <- c("Patient_ID", "Therapeutic_Outcome_Final", "Total_Area_Ulcer",
                   "V2_Total_Area_Ulcer", "V3_Total_Area_Ulcer", "V4_Total_Area_Ulcer",
                   "V5_Total_Area_Ulcer")
ulcer_areas <- as.data.frame(traits[, ulcer_columns])
colnames(ulcer_areas) <- c("Patient_ID", "Cure/Fail", "Ulcer_Area_1","V2", "V3", "V4", "V5")

v1_ulcer_invalid_idx <- ulcer_areas[["Ulcer_Area_1"]] == 999.0 &
  !is.na(ulcer_areas[["Ulcer_Area_1"]])
ulcer_areas[v1_ulcer_invalid_idx, "Ulcer_Area_1"] <- NA

v2_ulcer_invalid_idx <- ulcer_areas[["V2"]] == 999.0 &
  !is.na(ulcer_areas[["V2"]])
ulcer_areas[v2_ulcer_invalid_idx, "V2"] <- NA

v3_ulcer_invalid_idx <- ulcer_areas[["V3"]] == 999.0 &
  !is.na(ulcer_areas[["V3"]])
ulcer_areas[v3_ulcer_invalid_idx, "V3"] <- NA

v4_ulcer_invalid_idx <- ulcer_areas[["V4"]] == 999.0 &
  !is.na(ulcer_areas[["V4"]])
ulcer_areas[v4_ulcer_invalid_idx, "V4"] <- NA

v5_ulcer_invalid_idx <- ulcer_areas[["V5"]] == 999.0 &
  !is.na(ulcer_areas[["V5"]])
ulcer_areas[v5_ulcer_invalid_idx, "V5"] <- NA
```

## Reshape the lesions and ulcers for plotting

I also have a slight aversion to NSE in R, but I have learned to get
over myself.

```{r}
lesion_areas_plotting <- tidyr::pivot_longer(lesion_areas, Lesion_Area_1:V5,
                                             names_to = "Visit", values_to = "Measurement")
ulcer_areas_plotting <- tidyr::pivot_longer(ulcer_areas, Ulcer_Area_1:V5,
                                            names_to = "Visit", values_to = "Measurement")
```

# Plot the lesion/ulcer areas

<span style="color:#006400">
Theresa made these connected lines in order to make it easier
to see that (as one would hope) lesions/ulcers get smaller over
treatment.
</span>

```{r}
ggplot(lesion_areas_plotting, aes(x = Visit,
                                  y = Measurement,
                                  col = as.factor(`Cure/Fail`),
                                  group = Patient_ID)) +
  geom_point() +
  geom_line(aes(mapping = Patient_ID)) +
  ggtitle("Lesion Area Per Patient Across Visits") +
  labs(color = "Condition") +
  scale_shape_discrete(labels = c("Cure", "Therapeutic Failure", "Lost to Follow-up", "Excluded During Study")) +
  scale_color_discrete(labels = c("Cure", "Therapeutic Failure", "Lost to Follow-up", "Excluded During Study"))
```

```{r}
ggplot(ulcer_areas_plotting, aes(x = Visit,
                                 y = Measurement,
                                 col = as.factor(`Cure/Fail`),
                                 group = Patient_ID)) +
  geom_point() +
  geom_line(aes(mapping = Patient_ID)) +
  scale_shape_discrete(labels = c("Cure", "Therapeutic Failure", "Lost to Follow-up", "Excluded During Study")) +
  scale_color_discrete(labels = c("Cure", "Therapeutic Failure", "Lost to Follow-up", "Excluded During Study")) +
  labs(color = "Condition") +
    ggtitle("Ulcer Area Per Patient Across Visits")
```

I am wondering what the requirements are which determine a therapeutic
failure from a cure? It doesn’t appear that lesion or ulcer size
dictates that, since we have therapeutic failures with ulcer/lesion
sizes which are comparable to the cures.

Additionally, it doesn’t look like there is a relationship between how
fast ulcer/lesion size decreases between visits and the cure/fail
status. we have some cures that have ulcer/lesion sizes decrease
gradually, and some fails that decrease rapidly between V2 and V3.

# Correlation Between Variables

Let’s look at which of these variables are highly correlated

<span style="color:#006400">
I think the logic of the following couple of blocks is ideally suited
as an addition to hpgltools as a function named something like
'crosscor_pdata' for use as an early check of a new dataset to ensure
that the likely models are full rank etc.

Also, I have a TODO entry to translate all the columns from the
metadata in the beginning of the entire process, as a result these
blocks will likely fail soon -- though perhaps not because I will
likely just cut/paste this section to the beginning of
01datastructures...
</span>

```{r}
wanted_traits <- c(
    "Sex", "Ethnicity", "Age", "Weight", "Height", "Previously_Diagnosed", "Evolution_Time",
    "Num_Active_Lesions", "V2_New_Lesions", "V3_New_Lesions", "V4_New_Lesions", "V5_New_Lesions",
    "Total_Area_Ulcer", "Total_Area_Lesion", "V2_Total_Area_Lesion", "V2_Total_Area_Ulcer",
    "V3_Total_Area_Lesion", "V3_Total_Area_Ulcer", "V4_Total_Area_Lesion", "V4_Total_Area_Ulcer",
    "V5_Total_Area_Lesion", "V5_Total_Area_Ulcer", "Adherence", "Therapeutic_Outcome_V3",
    "Therapeutic_Outcome_V4", "Therapeutic_Outcome_Final")

interesting_traits <- traits[, wanted_traits]

interesting_traits[interesting_traits == 999.0] <- NA
#interesting_traits$Final_Status <- as.factor(interesting_traits$Final_Status)

interesting_traits <- interesting_traits %>%
    subset(Therapeutic_Outcome_Final != 3)
```

This first plot shows the correlations between all features I’ve
deemed “interesting”, and this includes all Final_Status samples
(cure/fail/lost/excluded).

```{r}
corr_cross(interesting_traits, # name of dataset
           max_pvalue = 0.05, # display only significant correlations (at 5% level)
           top = 20 # display top 10 couples of variables (by correlation coefficient)
           )
```

Interesting here that among the top 20 cross-correlations, the 17th
one (3rd from the bottom) is V1_Lesion_Area and Final_status with a
correlation of .623. This is a good sign that the visit 1 measurements
w(and hopefully that extends to expression profiles?) can help us
predict the final outcome of cure vs fail to cure.

I want to make sure the samples that are excluded aren’t screwing up
the correlations, so this following plot is the same as above, except
now I’ve subsetted the samples to only include those that are
cure/fail final status.

(Another note from trey): Theresa's notebook explicitly talks about
the lost samples, in most instances in the container these have been
pwruned; so they are unlikely to be included in my version of this.

```{r}
table(interesting_traits[["Therapeutic_Outcome_Final"]])
```

When we subset, we have 14 cures and 10 fail to cures, 3 lost.

```{r}
corr_cross(interesting_traits, max_pvalue = 0.2)

all_traits <- pData(tc_clinical)
test_traits <- all_traits[, c("clinic", "sex", "Ethnicity", "Age")]
test_traits[["sex"]] <- as.numeric(test_traits[["sex"]])
test_traits[["Ethnicity"]] <- as.numeric(test_traits[["Ethnicity"]])
corr_cross(test_traits, max_pvalue = 0.5)

wanted <- c("clinic", "sex", "Ethnicity", "Age", "Therapeutic_Outcome_Final")
funkytown <- all_traits[, wanted]
fct <- c("clinic", "sex", "Ethnicity", "Therapeutic_Outcome_Final")
for (f in fct) {
  funkytown[[f]] <- as.numeric(as.factor(funkytown[[f]]))

}
ggstatsplot::ggcorrmat(funkytown, label = TRUE, cor.vars = wanted)
```

This subsetting brings up some more interesting correlations.

### Showing how this function is coming up with correlations for categorical vs continuous variables

This method is producing correlation values between all pairwise
comparisons of variables. To do this between categorical and
continuous variables (such as Final_Status and V1_Lesion_Area), what
this does is first model the relationship between the continuous and
categorical variables as a regression, with the formula: continuous ~
categorical. Next, we can take the square root of the r.squared value
from this formula to be equal to the correlation coefficient (which is
basically a measure of how much of the variance is explained in the
continuous variable from the categorical variable).

As proof, here is the method broken down to get to the same corr value
in the plot above between V1_Lesion_Area and Final_status (.429 for
the cure/fail/lost samples).

```{r}
Y <- interesting_traits_CF$V1_Lesion_Area
X <- interesting_traits_CF$Final_Status
modelXY <- lm(Y ~ X)
## model summary
sumryXY <- summary(modelXY)
## r-sq of model
rsqXY <- sumryXY$r.squared
print(sqrt(rsqXY))
```

# TODO 202407 in meeting

Use a version of this with outcome ~ factor1+factor2+factor3... for
the set of putatively (un)interesting factors that are not confounded
and extract from the lm and perhaps an associated anova the beta and
pvalue; given this information create the vertical line+digression
plot Najib showed me and Theresa described. Example here:

https://medium.com/@dlab-berkeley/using-forest-plots-to-report-regression-estimates-a-useful-data-visualization-technique-2511491763f2

Path of reasoning to show in the document:

a.  Given only the Tumaco data, perform ggcorrmat of the metadata
    given a series of interesting metadata factors: sex, ethnicity,
    outcome (others?)
b.  Provide the resulting correlation plot of the Tumaco data.
c.  We must show explicitly that age and ethnicity are confounded
    (assuming they are), then we can add one or the other to the c/f
    model.
d.  Define 'highly correlated' as >= 0.65
e.  Consider how to generalize this reasoning to make an early
    diagnostic for the future.
f.  Generate the resulting models and do the lm() to get the inputs
    for a forest plot and create it -- only Tumaco?

1.  Show the correlation of Ethnicity/Sex/Age and highlight the
    resulting confounded state of age and ethnicity.
2.  Create a model of outcome ~ one of them (or both one at a time)
    along with the other factors of interest
3.  Given that model and its lm result, print out the table of
    resulting estimates and p-values
4.  Plot the resulting forest plot given #3

Forest plot

```{r}
fstring <- "Therapeutic_Outcome_Final ~ sex + Ethnicity + Age"
formula <- as.formula(fstring)
relationship <- lm(formula, data = pData(tc_clinical))
relationship
summary(relationship)

fstring <- "Therapeutic_Outcome_Final ~ Ethnicity + clinic + sex + Age"
formula <- as.formula(fstring)
test_relationship <- lm(formula, data = pData(tc_clinical))
test_relationship
summary(test_relationship)

fstring <- "Therapeutic_Outcome_Final ~ Ethnicity + Age"
formula <- as.formula(fstring)
control_relationship <- lm(formula, data = pData(tc_clinical))
control_relationship
summary(control_relationship)

fstring <- "Therapeutic_Outcome_Final ~ Ethnicity + sex + clinic"
formula <- as.formula(fstring)
query_relationship <- lm(formula, data = pData(tc_clinical))
query_relationship
summary(query_relationship)

```

I have done some additional filtering here. After speaking with Ade at
our meeting 9/8, we think it’s a good idea to drop the ‘lost’ samples
and only include known cure/fails. This will boil our correlation
metric down to

Let’s look at all pairwise comparisons with Final_Status to see which
of these metadata features is most correlated with it.

```{r}
interesting_traits_CFO <- interesting_traits_CF %>%
  subset(Therapeutic_Outcome_Final != 2)

corr_var(interesting_traits_CFO, # name of dataset
         Therapeutic_Outcome_Final, # name of variable to focus on
         top = 20 # display top 5 correlations
         )
```

is still of the highest correlations to Final_Status. (Keep in mind
when thinking about the V4_Lesion_Area result, we only have a few
samples which have this measurement, so it isn’t the most reliable).

Let’s visualize the difference in V1_Lesion Areas between
Cures/Fails/Lost

```{r}
plot(as.factor(interesting_traits_CF$Final_Status),
     interesting_traits_CF$V1_Lesion_Area,
     xlab = "Final Status", ylab = "V1 Lesion Area", xaxt = "n")
axis(1, at=1:3, labels=c("Cure", "Fail", "Lost"))
```

So it looks like the Fails have larger V1_Lesion_Areas at the time
point 1 visit compared to the cures.

Could there be a contributing factor here? Is this due to maybe the
people who fail are further along in disease progression than the ones
that cure? And so the cures actually cure because they start treatment
earlier?

```{r}
plot(as.factor(interesting_traits_CF$Final_Status),
     interesting_traits_CF$Evolution_time,
     xlab = "Final Status", ylab = "Evolution Time", xaxt = "n")
axis(1, at = 1:3, labels=c("Cure", "Fail", "Lost"))
```

It was determined that evolution time is measured in weeks. This plot
shows that the larger lesions on people are not due to them having
longer to progress. In fact, the fails have much shorter evolution
times.

So could this be opposite? Could the cures actually cure because the
disease is in later stages (later evolution times)? If we checked in
on the fails at the same evolution time that we did for the cures,
would they also eventually cure?

# Theresa's second notebook

The following uses much the same dataset, but uses a series of
logistic regression and partition trees to examine how various
metadata are related (or not) to cure/fail.

# CART

CART to choose the best features (genes) which classify clinical
outcome based on which genes are DE within each cell type.

First we need a list of all genes which are sig DE between cures/fails
within each type of cell.

<span style="color:#006400">
I get a kick out of some of these ML methods, regression trees are (I
think) a pretty direct descendent of nearest neighbor tree
construction; which, if you try to read some of the early papers
(@coverNearestNeighborPattern1967) about is pretty rough going; but
these are techniques that are very much not new.  I mean, geez, 'CART'
(@breimanClassificationRegressionTrees2017) is from the peak of the
unix wars in the eighties...  So the next time some tech bro talks up
tree-based model, get that person a shirt with some big shoulder pads.
Don't get me wrong, I love old computing; I have joyfully played with
HP apollo and DEC ultrix workstations from ~ 1985, but I never claimed
that they were the hot new thing.
</span>

## Feature selection for CART (DE)

The CART is built by the following process: first the single variable
is found which best splits the data into two groups (‘best’ is defined
by maximizing the gini index). The data is separated, and then this
process is applied separately to each sub-group, and so on recursively
until the subgroups either reach a minimum size (5 for this data) or
until no improvement can be made.

The gini index is a measure of how “pure” or
homogeneous a population is. So for example, if we had 2 populations
consisting of both men and women, but the proportions for each is as
follows: group 1: 98% men, 2% women group2: 50% men, 50% women,

<span style="color:#006400">
The gini index (I still think of it as a measure of income inequality
has its own neat history (@cerianiOriginsGiniIndex2012))
</span>

the Gini index would be practically 1 (on a scale of 0-1) for group 1
since it contains almost all of the same “class”, and the gini index
would be 0 for group 2 since it is perfectly non-homogeneous.

I will perform a DE for the V1 samples within each type of cell to
make a master list of genes we want to include in the CART.

<span style="color:#006400">
Theresa performed a smart quick trick here (which, having read it, I
did as well) and just loaded the xlsx file saved in the working tree.
If that DE savefile does prove to be obnoxious, I will just repeat
this for the other tasks.  However, there is at least one important
difference:  I enforced a series of file location conventions on the
outputs.  One thing this convention did was to make clear if the DE
table is of only Tumaco or of Tumaco+Cali.  Given that the files
Theresa is reading appear to be before that was made explicitly clear,
I am not 100% certain that these tables are of Tumaco or Tumaco+Cali
(I am like 99% certain though, I am pretty sure I named the
Tumaco+Cali with a suffix of something like 'both').  Also, we later
changed the sheet name from 'failure_vs_cure' to 'outcome'.
</span>

```{r xlsx_prefix}
xlsx_prefix <- file.path("analyses", "4_tumaco", "DE_Cure_vs_Fail")
sheet_name <- "outcome"
```

```{r}
v1_mono_table <- file.path(xlsx_prefix, "Monocytes", glue("t_monocyte_cf_table_sva-v{ver}.xlsx"))
v1_mono_de <- readxl::read_xlsx(v1_mono_table, sheet = sheet_name, skip = 1)

v1_eo_table <- file.path(xlsx_prefix, "Eosinophils", glue("t_eosinophil_cf_table_sva-v{ver}.xlsx"))
v1_eos_de <- readxl::read_xlsx(v1_eo_table, sheet = sheet_name, skip = 1)

v1_neut_table <- file.path(xlsx_prefix, "Neutrophils", glue("t_neutrophil_cf_table_sva-v{ver}.xlsx"))
v1_neut_de <- readxl::read_xlsx(v1_neut_table, sheet = sheet_name, skip = 1)

v1_mono_de <- v1_mono_de %>%
  filter(abs(deseq_logfc) >= 1.0 & deseq_adjp <= 0.05) %>%
  select("ensemblgeneid", "deseq_logfc", "deseq_adjp")

v1_eos_de <- v1_eos_de %>%
  filter(abs(deseq_logfc) >= 1.0 & deseq_adjp <= 0.05) %>%
  select("ensemblgeneid", "deseq_logfc", "deseq_adjp")

v1_neut_de <- v1_neut_de %>%
  filter(abs(deseq_logfc) >= 1.0 & deseq_adjp <= 0.05) %>%
    select("ensemblgeneid", "deseq_logfc", "deseq_adjp")
```

## Create one dataset to make CART from

The conventions used to find the expression data structures have also
been changed slightly from the time when Theresa performed this in her
working tree.  Thus, instead of hs_valid (e.g. the valid human
samples), the likely appropriate datastructure is 't_clinical', which
stands for 'Tumaco clinical samples.'

Finally, the sample sheet was changed slightly because we got multiple
new clinical outcome columns, due to the fact that a couple of people
came back for an extra visit and were re-evaluated.  Thus, instead of
'clinicaloutcome', the appropriate column is 'finaloutcome'.  Thus I
did a s/clinicaloutcome/finaloutcome/g.


```{r}
de_genes_ls <- c(v1_mono_de[["ensemblgeneid"]],
                 v1_eos_de[["ensemblgeneid"]],
                 v1_neut_de[["ensemblgeneid"]])
de_genes <- de_genes_ls[!duplicated(de_genes_ls)]

subset_idx <- "visitnumber=='1'&condition!='notapplicable'&typeofcells!='Biopsy'&typeofcells!='Macrophages'&condition!='lost'&typeofcells!='undefined'&finaloutcome!='undefined'"
hs_cart_1 <- subset_expt(t_clinical, subset = subset_idx)

hs_cart_norm <- normalize_expt(hs_cart_1, transform = "log2", convert = "cpm",
                               norm = "quant")

hs_cart_2 <- as.data.frame(t(exprs(hs_cart_norm)[de_genes, ]))
hs_cart_2 <- merge(hs_cart_2, pData(hs_cart_norm)[hs_cart_norm$samplenames, c("finaloutcome", "typeofcells")],
                   by=0, all=TRUE)
```

## Build the CART using rpart()

<span style="color:#006400">
I think there are some interesting points of contact here with what I
did in my ML explorations.  It may work well to pull this into the
functions I wrote for that.  Also, keep in mind, rpart is a pretty
direct implementation of material from the CART book.
</span>

```{r}
hs_cart_2$finaloutcome <- tolower(hs_cart_2$finaloutcome)
## atb: I am not sure I understand the c(-1, -93, -94)
## gene_vars <- as.formula(paste0("finaloutcome ~ ",
## paste0(colnames(hs_cart_2)[c(-1, -93, -94)], collapse = " + ",
## recycle0 = FALSE)))
gene_vars <- as.formula(paste0("finaloutcome ~ ", paste0(colnames(hs_cart_2)[-1],
                                                         collapse = " + ", recycle0 = FALSE)))
## This is making a crazy-long model using lots of gene IDs but excluding a few?
## I must be misunderstanding the shape/intention of the original matrix
## Can one not make a similar model with just finaloutcome ~ .?  Oh,
## but to do so, one must exclude finaloutcome from the model matrix
## first.  ooooh ok, I get it -- I think we cannot assume that -1,
## -93, and -94 will work because we do not know that the set of DE
## genes will retain the same length from iteration to iteration (it
## will be close no doubt)  But the intention was to exclude a few
## things and create a model with ~ gene_ids + finaloutcome + typeofcells

# grow tree
fit <- rpart(gene_vars,
             method = "class", data = hs_cart_2)

printcp(fit) # display the results

summary(fit) # detailed summary of splits
```

## Plot the tree

<span style="color:#006400">
I like these plots.  In the following plot it appears to boil the
cure/fail distinction down to two genes: CCDC163 (a coiled coil domain
containing protein which may be transmembrane or perhaps a
pseudogene!?) and IRF7 (interferon regulatory factory 7 -- ok, that
one is cool)

This is a point where I will likely have to make some actual changes
to Theresa's work, she hard-coded some of the gene IDs to examine in
the following blocks, a couple of them are different now due to the
slight differences in our final dataset (e.g. I think we lost one more
person since she did this).
</span>

```{r}
rpart.plot(fit, extra = 2)
```

Each node shows:
1. The class with the largest proportions of it’s observations
2. how many samples are of that class within that node

This is the same tree just printed with different information.

<span style="color:#006400">
Here is my first actual change: I yanked out the top-three most
important genes from the rpart object and will be using them by the
variable's value in the following plots etc rather than by name
because they cannot be trusted to stay the same over time.
</span>

```{r}
fit_plot <- rpart.plot(fit)

## I will grab the top-3 three genes from the rpart object
top_three <- head(names(fit_plot[["obj"]][["variable.importance"]]), n = 3)
```

Each node shows
- the predicted class (cure/failure),
- the predicted probability of cure/fail/lost,
- the percentage of observations in the node.

## Close look at Expression of this predictor gene

Let’s plot the expression of this gene split by cure/fail

```{r}
p1 <- ggplot(hs_cart_2, aes(x = finaloutcome, y = !!sym(top_three[1]), color = typeofcells)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point() +
  geom_hline(yintercept = 7.030467, color = "red") +
  theme_classic()
p1

p2 <- ggplot(hs_cart_2, aes(x = finaloutcome, y = !!sym(top_three[2]), color = typeofcells)) +
  geom_point() +
  geom_hline(yintercept = 7.030467, color = "red") +
  theme_classic()
p2

p3 <- ggplot(hs_cart_2, aes(x = finaloutcome, y = !!sym(top_three[3]), color = typeofcells)) +
  geom_point() +
  geom_hline(yintercept = 7.030467, color = "red") +
  theme_classic()
p3
```

These are the same plot, the only difference is the one on the left
has the boxplots to show the breakup of cell types in each clinical
outcome group.

My original thought was that the genes which would show up as good
predictors of outcome would be the 3 DE genes which are in DE for all
different cell types. The overlapping genes are these:

<span style="color:#006400">
(note from trey: the results from my filtered run of this are
sufficiently different that I need to hunt down that intersection)...
</span>

And none of these are the one gene in the CART that showed up as a
good predictor. That’s interesting. The gene that did show up as a
good predictor (ENSG00000182162: P2RY8) is significantly DE in the
eosinophils and the LFC isn’t even that large:

```{r}
v1_eos_de[v1_eos_de$ensemblgeneid == "ENSG00000182162",]
```

We can see that the change in expression in the other two cell types
definitely shows the same trend though, although not significant. I
would guess this gene shows up in the DE comparison of all cell types
at once with cure/fail as the condition.

I am not wild about this result. Let’s do each cell type individually,
even though there won’t be many samples in each to use to build the
tree. This is “cheating” for ML people since I am doing no CV, no
train/test, so please don’t tell on me.

## Let's try a different normalization for fun

Will changing the count normalization change the optimal tree? Let’s
find out!

TMM normalization

```{r}
hs_cart_tmm <- normalize_expt(hs_cart_1, transform = "log2", convert = "cpm",
                              norm = "tmm")

hs_cart_3 <- as.data.frame(t(exprs(hs_cart_tmm)[de_genes,]))
hs_cart_3 <- merge(hs_cart_3, pData(hs_cart_tmm)[hs_cart_tmm$samplenames, c("finaloutcome", "typeofcells")],
                   by = 0, all = TRUE)

hs_cart_3[["finaloutcome"]] <- tolower(hs_cart_3[["finaloutcome"]])
gene_vars <- as.formula(paste0("finaloutcome ~ ", paste0(colnames(hs_cart_3)[-1],
                                                         collapse = " + ", recycle0 = FALSE)))

# grow tree
fit <- rpart(gene_vars,
             method = "class", data = hs_cart_2)

printcp(fit) # display the results

rpart.plot(fit, extra = 2)
rpart.plot(fit)
```

Interesting! Changing the normalization to tmm keeps the same first
gene for the initial split, but then it allows the “cure” group to get
split again to get even better splits. Let’s look at this new gene for
the samples which have tmm normalized expression of ENSG00000280670 >=
5.7

```{r}
hs_cart_2 %>%
  ggplot(aes(x = finaloutcome, y = !!sym(top_three[2]), color = typeofcells)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point() +
  geom_hline(yintercept = 3.8, color = "red") +
  theme_classic()  +
    theme(legend.position = "none")
```

Because I know someone will be interested, “ENSG00000203907” is OOEP:
oocyte expressed protein. So this seems odd? We’re probably just way
overfitting to our data.

Ideally, we would do this for just one of our cell types. But we
really just don’t have enough samples for that.

# CART using demographics data

There is an important change here, we stripped the demographics data
out of the expressionset.  Thus, in order to do this following block
we will need to bring it back.

```{r}
hs_pd <- pData(t_clinical)
traits[["join"]] <- tolower(traits[["codigo_paciente"]])
hs_pd[["join"]] <- hs_pd[["tubelabelorigin"]]
hs_pd_demographics <- plyr::join(hs_pd, traits, by = "join")
hs_pd_demographics[["join"]] <- NULL
rownames(hs_pd_demographics) <- rownames(hs_pd)
na_idx <- is.na(hs_pd_demographics)
sum(na_idx)
hs_pd_demographics[na_idx] <- "undefined"
colnames(hs_pd_demographics) <- make.names(colnames(hs_pd_demographics), unique = TRUE)
pData(t_clinical) <- hs_pd_demographics

factors_oi <- c("finaloutcome", "visitnumber", "drug", "eb_lc_sexo",
                "eb_lc_etnia",
                "edad",
                "eb_lc_peso",
                "eb_lc_estatura",
                "eb_lc_dx_previo",
                "eb_lc_tiempo_evolucion",
                "eb_lc_num_lc_activas",
                "v2_lc_num_lesiones_nuevas",
                "v3_lc_num_lesiones_nuevas",
                "v4_lc_num_lesiones_nuevas",
                "v5_lc_num_lesiones_nuevas",
                "eb_lc_ulcera_area_1",
                "eb_lc_lesion_area_1",
                "v2_lc_lesion_area_1",
                "v2_lc_ulcera_area_1",
                "v3_lc_lesion_area_1",
                "v3_lc_ulcera_area_1",
                "v4_lc_lesion_area_1",
                "v4_lc_ulcera_area_1",
                "v5_lc_lesion_area_1",
                "v5_lc_ulcera_area_1",
                "adherencia_tto",
                "v3_lc_rta_tto",
                "v4_lc_rta_tto",
                "v5_lc_rta_tto")

hs_int_fac <- pData(t_clinical) %>%
  select(all_of(factors_oi)) %>%
  filter(finaloutcome != "undefined")

colnames(hs_int_fac) <- c("finaloutcome", "visitnumber", "drug", "Sex",
                          "Ethnicity",
                          "Age",
                          "Weight",
                          "Height",
                          "Previously_diagnosed",
                          "Evolution_time",
                          "Num_Active_Lesions",
                          "V2_New_Lesions",
                          "V3_New_Lesions",
                          "V4_New_Lesions",
                          "V5_New_Lesions",
                          "V1_Ulcer_Area",
                          "V1_Lesion_Area",
                          "V2_Lesion_Area",
                          "V2_Ulcer_Area",
                          "V3_Lesion_Area",
                          "V3_Ulcer_Area",
                          "V4_Lesion_Area",
                          "V4_Ulcer_Area",
                          "V5_Lesion_Area",
                          "V5_Ulcer_Area",
                          "Adherence",
                          "V3_Therapeutic_Resp",
                          "V4_Therapeutic_Resp",
                          "V5_Therapeutic_Resp")

#convert to numerical
to_convert <- c("Age", "Weight", "Height", "Evolution_time", "Num_Active_Lesions", "V2_New_Lesions", "V3_New_Lesions", "V4_New_Lesions", "V5_New_Lesions", "V1_Ulcer_Area", "V1_Lesion_Area", "V2_Lesion_Area", "V2_Ulcer_Area", "V3_Lesion_Area", "V3_Ulcer_Area", "V4_Lesion_Area", "V4_Ulcer_Area", "V5_Lesion_Area", "V5_Ulcer_Area", "Adherence", "V3_Therapeutic_Resp", "V4_Therapeutic_Resp", "V5_Therapeutic_Resp")

for (i in to_convert) {
  hs_int_fac[[i]] <- as.numeric(hs_int_fac[[i]])
}

hs_int_fac[hs_int_fac == 999.0] <- NA # set all the '999' to NA
hs_int_fac$finaloutcome <- tolower(hs_int_fac$finaloutcome)
hs_int_fac <- hs_int_fac %>%
  distinct()
head(hs_int_fac[,c(1:5)])
```

## Build the CART

```{r}
vars_formula <- as.formula(paste0("finaloutcome ~ ",
                                  paste0(colnames(hs_int_fac)[c(-1)],
                                         collapse = " + ", recycle0 = FALSE)))

# grow tree

fit <- rpart(vars_formula,
             method = "class", data = hs_int_fac)

printcp(fit) # display the results

summary(fit)
```

## Plot the tree

```{r}
rpart.plot(fit, extra = 2)
rpart.plot(fit)
```

## Closer look at these variables

```{r}
p1 <- ggplot(hs_int_fac, aes(x = finaloutcome, y = Age, color = Evolution_time)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(size = 2) +
  geom_hline(yintercept = 29, color = "red") +
  ylim(c(10,55)) +
  theme_classic()  +
  annotate(geom="text", x="failure", y=40, label="Cures",
           color="red")  +
  annotate(geom="text", x="failure", y=15, label="Splits Again",
           color="red")

p1

hs_int_fac %>%
  filter(Age < 29) %>%
  ggplot(aes(x = finaloutcome, y = Evolution_time)) +
  geom_point() +
  geom_hline(yintercept = 7, color = "red") +
  theme_classic()   +
  ylim(c(0,23)) +
  annotate(geom="text", x="failure", y=13, label="Cures",
           color="red")  +
  annotate(geom="text", x="failure", y=1, label="Failure",
           color="red")
```

# SVA Loadings

<span style="color:#006400">
Note from atb: I need to make a few changes to this section, primarily
we need to be able to automatically generate the tables of
f-statistics; in case the data changes (which it did since Theresa
performed this, one sample was removed I think).  With that caveat,
the following is coming directly out of her SVA_V3_Tumaco document.  I
also would like to compare the SV-fstats to similar metrics I took of
PCs vs. metadata factors.  My assumption (if I understand the math in
sva at all) is that they should largely complement/agree with each
other.
</span>

We would like to know what the heck SVA is actually correcting for
when we do an SVA correction. Are there any metadatas that these SV’s
are correlated with?

To do this, I will run SVA to get the SV loadings. I will then do
something akin to PC loadings analysis to see how these individual SVs
(and combinatorial SVs) are associated with any

I will use a computed F-statistic for this association to measure the
between:within cluster variance in a model (and tell us if that factor
is a “good” indicator of separation based on that sv loading).

$$\begin{equation}
F-statistic = \frac{TSS - RSS}{RSS}
\end{equation}$$

So for this, I will use a series of linear regressions which model
each dimension of SVA as a function of the observed variables that
describe the known underlying group structure (clinic, visit, patient,
...)

$$\begin{equation}
\underbrace{X_i}_\text{dimension i of SVA} = \underbrace{B_0 + B_1
celltype/visit/clinic/donor}_\text{underlying group structure}
\end{equation}$$

We can do this breakdown in a few ways to answer different questions
which I will explore further below.

We have decided the Cali samples don't offer a lot of extra
information for us, and there is significant clinic batch effect, so
we are going to remove the Cali samples and evaluate the SV loadings.

The first thing to do is the actual SVA to get the loadings.

<span style="color:#006400">
I may have changed a few of Theresa's variable names when I first
copy/pasted this document together without taking note of the
modification; but I am reasonably certain that the intended data
structures are the same.
</span>

```{r}
clinic_sva <- normalize_expt(t_clinical, filter = TRUE)
pheno <- pData(clinic_sva)
edata <- exprs(clinic_sva)

mod <- model.matrix(~as.factor(finaloutcome), data = pheno)
mod0 <- model.matrix(~1, data = pheno)

svobj <- sva::svaseq(edata, mod, mod0)
```

### SV 1

SVA found 4 SV’s. We can plot them individually to visually inspect
their separation w.r.t some metadata.

```{r}
svs <- as.data.frame(svobj$sv)
colnames(svs) <- paste0("sv_", seq(1:4))
svs <- cbind(svs, pheno)

sv1_typeofcells <- ggplot(svs, aes(y = sv_1, x = typeofcells, fill = typeofcells)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Type of Cells") +
  ylab("SV 1")  +
  theme_classic() +
  theme(legend.position = "none")

sv1_visit <-  ggplot(svs, aes(y = sv_1, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 1")  +
  theme_classic() +
  theme(legend.position = "none")

sv1_donor <- ggplot(svs, aes(y = sv_1, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Donor") +
  ylab("SV 1")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))

sv1_typeofcells
sv1_visit
sv1_donor
##grid.arrange(sv1_typeofcells, sv1_visit, sv1_donor, nrow = 2)
```

### SV2

```{r}
sv2_typeofcells <- ggplot(svs, aes(y = sv_2, x = typeofcells, fill = typeofcells)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Type of Cells") +
  ylab("SV 2")  +
  theme_classic() +
  theme(legend.position = "none")

sv2_visit <-  ggplot(svs, aes(y = sv_2, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 2")  +
  theme_classic() +
  theme(legend.position = "none")

sv2_donor <- ggplot(svs, aes(y = sv_2, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Donor") +
  ylab("SV 2")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))


#grid.arrange(sv2_typeofcells, sv2_visit, sv2_donor, nrow = 2)
sv2_typeofcells
sv2_visit
sv2_donor
```

### SV3

```{r}
sv3_typeofcells <- ggplot(svs, aes(y = sv_3, x = typeofcells, fill = typeofcells)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Type of Cells") +
  ylab("SV 3")  +
  theme_classic() +
  theme(legend.position = "none")

sv3_visit <-  ggplot(svs, aes(y = sv_3, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 3")  +
  theme_classic() +
  theme(legend.position = "none")

sv3_donor <- ggplot(svs, aes(y = sv_3, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Donor") +
  ylab("SV 3")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))


##grid.arrange(sv3_typeofcells, sv3_visit, sv3_donor, nrow = 2)
sv3_typeofcells
sv3_visit
sv3_donor
```

### SV4

```{r}
sv4_typeofcells <- ggplot(svs, aes(y = sv_4, x = typeofcells, fill = typeofcells)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Type of Cells") +
  ylab("SV 4")  +
  theme_classic() +
  theme(legend.position = "none")

sv4_visit <-  ggplot(svs, aes(y = sv_4, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 4")  +
  theme_classic() +
  theme(legend.position = "none")

sv4_donor <- ggplot(svs, aes(y = sv_4, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = .75) +
  xlab("Donor") +
  ylab("SV 4")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))


##grid.arrange(sv4_typeofcells, sv4_visit, sv4_donor, nrow = 2)

sv4_typeofcells
sv4_visit
sv4_donor
```

SV 4 again seems to account for some donor separation.

### What variance does each SV account for?

We want to know what metadata each SV is associated with. For this, I
will use the F-statistic described above.

```{r, eval=FALSE}
sv <- svobj$sv
#getting Residual sum of squares for each SV
RSS_toc <- c()
test_toc <- c()
test_rss <- c()
for (i in 1:4) {
  test_lm <- lm(as.numeric(sv[, i]) ~ as.factor(pheno$typeofcells))
  test_anova <- anova(test_lm)
  test_toc[i] <- test_anova[2, 2]
  RSS_toc[i] <- anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$typeofcells)))[2, 2]
  test_rss[i] <- sum(test_anova[, 2])
}
RSS_vn <- c()
for (i in 1:4) {
  RSS_vn[i] <- anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$visitnumber)))[2, 2]
}
RSS_dr <- c()
for (i in 1:4) {
  RSS_dr[i] <- anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$donor)))[2, 2]
}

#getting total sum of squares for each SV
TSS_toc = c()
for (i in 1:4) {
  TSS_toc[i] <- sum(anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$typeofcells)))[, 2])
}
TSS_vn = c()
for (i in 1:4) {
  TSS_vn[i] <- sum(anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$visitnumber)))[, 2])
}
TSS_dr = c()
for (i in 1:4) {
  TSS_dr[i] <- sum(anova(lm(as.numeric(sv[, i]) ~ as.factor(pheno$donor)))[, 2])
}

Fstat_toc <- c((TSS_toc[1] - RSS_toc[1]) / RSS_toc[1],
               (TSS_toc[2] - RSS_toc[2]) / RSS_toc[2],
               (TSS_toc[3] - RSS_toc[3]) / RSS_toc[3],
               (TSS_toc[4] - RSS_toc[4]) / RSS_toc[4])

Fstat_vn <- c((TSS_vn[1] - RSS_vn[1]) / RSS_vn[1],
              (TSS_vn[2] - RSS_vn[2]) / RSS_vn[2],
              (TSS_vn[3] - RSS_vn[3]) / RSS_vn[3],
              (TSS_vn[4] - RSS_vn[4]) / RSS_vn[4])

Fstat_dr <- c((TSS_dr[1] - RSS_dr[1]) / RSS_dr[1],
              (TSS_dr[2] - RSS_dr[2]) / RSS_dr[2],
              (TSS_dr[3] - RSS_dr[3]) / RSS_dr[3],
              (TSS_dr[4] - RSS_dr[4]) / RSS_dr[4])

Fstats <- data.frame("F_stat_celltype" = round(Fstat_toc, 4),
                     "F_stat_visit" = round(Fstat_vn, 4),
                     "F_stat_donor" = round(Fstat_dr, 4))
rownames(Fstats) <- c("SV_1", "SV_2", "SV_3", "SV_4")
Fstats
```

<span style="color:#006400">
I spent a little time to simplify and try to make the reasoning above
a little more robust so that I can regenerate Theresa's xlsx table of
f-statistics as well as add a little more information.  The following
block attempts this...
</span>

Najib correctly pointed out that I left off clinic in this first invocation.

```{r}
queries <- c("typeofcells", "visitnumber", "clinic", "donor")
tc_clinical_fpstats <- svpc_fstats(tc_clinical, num_pcs = 5, queries = queries)
queries <- c("typeofcells", "visitnumber", "donor")
t_clinical_fpstats <- svpc_fstats(t_clinical, num_pcs = 5, queries = queries)
c_clinical_fpstats <- svpc_fstats(c_clinical, num_pcs = 5, queries = queries)
```

# Send to an xlsx workbook

<span style="color:#006400">
I am going to add a little code in this section to send this to an
xlsx file.  I might need to add a little bit of code as well because I
am not certain that there is a document which contains this
calculation for each data subset.

I put together a quick function which writes the results of one of
these analyses to a xlsx file, but it very much assumes a single
dataset and is not easily amendable to multiple; therefore I will
strip the code out here into a new function to repeat itself for the
Tumaco/Cali/Both data for an arbitrary combination.
</span>

Query from Maria Adelaida: Perform a similar f/p statistics plot/xlsx
table but using the first 5 PCs and SVs; perhaps also include the
amount of variance remaining tale (I forget its name: residuals).

But also do slightly different plots: 2 plots: 1 with PCs before SVA
followed by the SVs, the 1 with SVs followed by post PCs.

Given this, perform this task with: Clinic, Donor, Visit, Celltype
using the clinical samples (no biopsies).

```{r}
write_combined_fpstats <- function(both = tc_clinical_fpstats, tumaco = t_clinical_fpstats,
                                   cali = c_clinical_fpstats,
                                   excel = "excel/combined_svpc_fstats.xlsx") {
  xlsx <- init_xlsx(excel)
  wb <- xlsx[["wb"]]
  excel_basename <- xlsx[["basename"]]
  do_excel <- TRUE
  if (is.null(wb)) {
    do_excel <- FALSE
  }

  current_row <- 1

  pref <- both[["pre_f"]]
  svf <- both[["sv_f"]]
  postf <- both[["post_f"]]
  ## Changing the rownames due to rbind rownames shenanigans.
  rownames(pref) <- paste0("PrePC", seq_len(nrow(pref)))
  rownames(postf) <- paste0("PostPC", seq_len(nrow(postf)))
  allf <- rbind(pref, svf, postf)

  prep <- both[["pre_p"]]
  svp <- both[["sv_p"]]
  postp <- both[["post_p"]]
  rownames(prep) <- paste0("PrePC", seq_len(nrow(prep)))
  rownames(postp) <- paste0("PostPC", seq_len(nrow(postp)))
  allp <- rbind(prep, svp, postp)

  fun_plot <- heatmap.3(as.matrix(allp), dendrogram = "none",
                        scale = "none", trace = "none",
                        Colv = FALSE, Rowv = FALSE)
  image <- grDevices::recordPlot()

  xlsx_result <- write_xlsx(data = allf, wb = wb, sheet = "Fvalues", start_row = current_row,
                            title = "Both clinics, SVA and PC analysis, F-values")
  xlsx_result <- write_xlsx(data = allp, wb = wb, sheet = "Pvalues", start_row = current_row,
                            title = "Both clinics, SVA and PC analysis, P-values")
  current_row <- xlsx_result[["end_row"]] + 2
  try_result <- xlsx_insert_png(
    a_plot = image, wb = wb, sheet = "Pvalues", start_col = ncol(allp) + 2)
  image_files = c()
  if (! "try-error" %in% class(try_result)) {
    image_files = try_result[["filename"]]
  }

  pref <- tumaco[["pre_f"]]
  svf <- tumaco[["sv_f"]]
  postf <- tumaco[["post_f"]]
  ## Changing the rownames due to rbind rownames shenanigans.
  rownames(pref) <- paste0("PrePC", seq_len(nrow(pref)))
  rownames(postf) <- paste0("PostPC", seq_len(nrow(postf)))
  allf <- rbind(pref, svf, postf)

  prep <- tumaco[["pre_p"]]
  svp <- tumaco[["sv_p"]]
  postp <- tumaco[["post_p"]]
  rownames(prep) <- paste0("PrePC", seq_len(nrow(prep)))
  rownames(postp) <- paste0("PostPC", seq_len(nrow(postp)))
  allp <- rbind(prep, svp, postp)

  xlsx_result <- write_xlsx(data = allf, wb = wb, sheet = "Fvalues", start_row = current_row,
                            title = "Tumaco, SVA and PC analysis, F-values")
  xlsx_result <- write_xlsx(data = allp, wb = wb, sheet = "Pvalues", start_row = current_row,
                            title = "Tumaco, SVA and PC analysis, P-values")
  current_row <- xlsx_result[["end_row"]] + 2

  pref <- cali[["pre_f"]]
  svf <- cali[["sv_f"]]
  postf <- cali[["post_f"]]
  ## Changing the rownames due to rbind rownames shenanigans.
  rownames(pref) <- paste0("PrePC", seq_len(nrow(pref)))
  rownames(postf) <- paste0("PostPC", seq_len(nrow(postf)))
  allf <- rbind(pref, svf, postf)

  prep <- cali[["pre_p"]]
  svp <- cali[["sv_p"]]
  postp <- cali[["post_p"]]
  rownames(prep) <- paste0("PrePC", seq_len(nrow(prep)))
  rownames(postp) <- paste0("PostPC", seq_len(nrow(postp)))
  allp <- rbind(prep, svp, postp)

  xlsx_result <- write_xlsx(data = allf, wb = wb, sheet = "Fvalues", start_row = current_row,
                            title = "Cali, SVA and PC analysis, F-values")
  xlsx_result <- write_xlsx(data = allp, sheet = "Pvalues", wb = wb, start_row = current_row,
                            title = "Cali, SVA and PC analysis, P-values")
  current_row <- xlsx_result[["end_row"]] + 2

  excel_ret <- try(openxlsx::saveWorkbook(wb, excel, overwrite = TRUE))
  removed <- try(suppressWarnings(file.remove(image_files)), silent = TRUE)
}

clinical_fpstats <- write_combined_fpstats(
  both = tc_clinical_fpstats, tumaco = t_clinical_fpstats, cali = c_clinical_fpstats,
  excel = glue("excel/clinical_fpstats-v{ver}.xlsx"))
```

The F-stat resulting from an anova for the model sv ~ metadata_factor
shows that the main thing we are correcting for with an SVA correction
(with cure/fail as the model factor) is the cell type. The factor
donor contributes the next highest separation, with clinic falling in
third. the visit contributes essentially no variance in this data,
which we knew from the DE results.

I am interested to see what happens if we repeat this analysis, but
for within each cell type. Will the clinic separation show up more in
those SVs, or are we really correcting more for donor? Let’s check it
out.

### Monocyte SVA

```{r}
t_mono_filt <- normalize_expt(t_monocytes, filter = TRUE)
pheno_mono <- pData(t_mono_filt)
edata_mono <- exprs(t_mono_filt)

mod <- model.matrix(~as.factor(finaloutcome), data = pheno_mono)
mod0 <- model.matrix(~1, data = pheno_mono)

svobj <- sva::svaseq(edata_mono, mod, mod0)
```

2 SVs found for monocytes.

#### SV 1

```{r}
svs <- as.data.frame(svobj$sv)
colnames(svs) <- paste0("sv_", seq(1:2))
svs <- cbind(svs, pheno_mono)

sv1_visit <-  ggplot(svs, aes(y = sv_1, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 1")  +
  theme_classic() +
  theme(legend.position = "none")

sv1_donor <- ggplot(svs, aes(y = sv_1, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Donor") +
  ylab("SV 1")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))

grid.arrange(sv1_visit, sv1_donor, nrow = 1)
```

#### SV2

```{r}
sv2_visit <-  ggplot(svs, aes(y = sv_2, x = visitnumber, fill = visitnumber)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Visit Number") +
  ylab("SV 2")  +
  theme_classic() +
  theme(legend.position = "none")

sv2_donor <- ggplot(svs, aes(y = sv_2, x = donor, fill = donor)) +
  geom_violin() +
  geom_point(alpha = 0.75) +
  xlab("Donor") +
  ylab("SV 2")  +
  theme_classic() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))

grid.arrange(sv2_visit, sv2_donor, nrow = 1)
```

<span style="color:#006400">
This marks the end of Theresa's publicly available notebooks in this
context.  I did a little looking in her scratch tree and found the
likely source documents for the xlsx document she generated.
These documents have the prefix 'current_tmrc3_SVA_*' and appear at
first glance to repeat the above for a series of clinic and celltype
subsets.  I am very close the certain that if I attempt to copy/paste
the full range of groups and subsets it will end in
typeographical-error shenanigans.  As a result I am going to
turn it into a simple function and parameterize it.
</span>

# Bibliography
